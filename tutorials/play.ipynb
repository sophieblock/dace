{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d837da",
   "metadata": {},
   "source": [
    "# DaCe IR Deep Dive: LibraryNodes and Operator Abstractions\n",
    "\n",
    "This notebook demonstrates how DaCe's Intermediate Representation (IR) handles operator abstractions through LibraryNodes. We'll explore:\n",
    "\n",
    "1. **LibraryNodes**: High-level operator abstractions in the IR\n",
    "2. **Semantic Contracts**: How `@dace.library.node` defines compile-time \"facts\"\n",
    "3. **Implementation Registry**: How concrete implementations are attached to abstract operators\n",
    "4. **Expansion System**: How abstract operators become concrete code\n",
    "\n",
    "## What are LibraryNodes?\n",
    "\n",
    "LibraryNodes are DaCe's way of representing high-level operations (like GEMM, FFT, reductions) as single nodes in the SDFG. They serve as:\n",
    "- **Abstractions**: Hide implementation complexity behind semantic interfaces\n",
    "- **Contracts**: Define what the operation does, not how it's implemented\n",
    "- **Extension Points**: Allow multiple implementations for different targets (CPU, GPU, FPGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ca2c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11\n",
      " CONDA prefix: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev\n",
      "Listing known BLAS/LAPACK libs under /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/lib\n",
      "  libblas*: 2\n",
      "   - libblas.3.dylib\n",
      "   - libblas.dylib\n",
      "  libcblas*: 2\n",
      "   - libcblas.3.dylib\n",
      "   - libcblas.dylib\n",
      "  liblapack*: 2\n",
      "   - liblapack.3.dylib\n",
      "   - liblapack.dylib\n",
      "  liblapacke*: 0\n",
      "  libopenblas*: 7\n",
      "   - libopenblas.0.dylib\n",
      "   - libopenblas.a\n",
      "   - libopenblas.dylib\n",
      "   - libopenblas_armv8p-r0.3.30.dylib\n",
      "   - libopenblas_vortexp-r0.3.30.a\n",
      "   ...\n",
      "  libmkl*: 0\n",
      "\n",
      "ctypes: 1.1.0\n",
      "ctypes.find_library...\n",
      "  blas: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/../lib/libblas.dylib\n",
      "  cblas: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/../lib/libcblas.dylib\n",
      "  lapacke: None\n",
      "  lapack: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/../lib/liblapack.dylib\n",
      "  openblas: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/../lib/libopenblas.dylib\n",
      "  mkl_rt: None\n",
      "\n",
      "DaCe version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "import dace\n",
    "import os\n",
    "import dace.library\n",
    "import dace.properties\n",
    "import dace.sdfg.nodes\n",
    "from dace.transformation.transformation import ExpandTransformation\n",
    "from dace import SDFG, SDFGState, symbolic, dtypes\n",
    "import numpy as np\n",
    "from dace.config import Config\n",
    "import ctypes.util, os,sys\n",
    "print('Python:', sys.version.split()[0])\n",
    "cp = os.environ.get('CONDA_PREFIX')\n",
    "print(' CONDA prefix:', cp)\n",
    "lib = os.path.join(cp, 'lib')\n",
    "\n",
    "print('\\nctypes:', ctypes.__version__)\n",
    "print('ctypes.find_library...')\n",
    "for name in ['blas','cblas','lapacke','lapack','openblas','mkl_rt']:\n",
    "    print(f'  {name}:', ctypes.util.find_library(name))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nDaCe version: {dace.__version__}\")\n",
    "\n",
    "current_conf = Config._config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08c2b232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'debugprint': 'verbose',\n",
       " 'frontend': {'cache_size': 0,\n",
       "  'implicit_recursion_depth': 64,\n",
       "  'raise_nested_parsing_errors': False,\n",
       "  'verbose_errors': False,\n",
       "  'preprocessing_passes': 5,\n",
       "  'dont_fuse_callbacks': False,\n",
       "  'typed_callbacks_only': False,\n",
       "  'unroll_threshold': -1,\n",
       "  'check_args': False,\n",
       "  'avoid_wcr': False},\n",
       " 'compiler': {'build_type': 'Debug',\n",
       "  'codegen_lineinfo': True,\n",
       "  'extra_cmake_args': '-DBLA_VENDOR=OpenBLAS -DCMAKE_FIND_FRAMEWORK=LAST -DCMAKE_INCLUDE_PATH=${CONDA_PREFIX}/include -DCMAKE_LIBRARY_PATH=${CONDA_PREFIX}/lib',\n",
       "  'cpu': {'executable': '',\n",
       "   'args': '-std=c++14 -fPIC -Wall -Wextra -O3 -march=native -fno-strict-aliasing -Wno-unused-parameter -Wno-unused-label -I/opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/include',\n",
       "   'openmp_sections': False,\n",
       "   'libs': ''},\n",
       "  'linker': {'args': '', 'executable': ''},\n",
       "  'cuda': {'default_block_size': '64,8,1',\n",
       "   'backend': 'auto',\n",
       "   'path': '',\n",
       "   'args': '-Xcompiler -march=native --use_fast_math -Xcompiler -Wno-unused-parameter',\n",
       "   'hip_args': '-std=c++17 -fPIC -O3 -ffast-math -Wno-unused-parameter',\n",
       "   'cuda_arch': '60',\n",
       "   'hip_arch': 'gfx906',\n",
       "   'dynamic_map_block_size': '128,1,1',\n",
       "   'dynamic_map_fine_grained': True,\n",
       "   'persistent_map_SM_fraction': 1.0,\n",
       "   'persistent_map_occupancy': 2,\n",
       "   'max_concurrent_streams': 0,\n",
       "   'syncdebug': False,\n",
       "   'libs': '',\n",
       "   'mempool_release_threshold': -1,\n",
       "   'block_size_limit': 1024,\n",
       "   'block_size_lastdim_limit': 64,\n",
       "   'thread_id_type': 'int32',\n",
       "   'allow_implicit_memlet_to_map': True},\n",
       "  'use_cache': False,\n",
       "  'library_prefix': 'lib',\n",
       "  'library_extension': 'dylib',\n",
       "  'indentation_spaces': 4,\n",
       "  'allow_shadowing': True,\n",
       "  'codegen_state_struct_suffix': '_state_t',\n",
       "  'format_code': False,\n",
       "  'format_config_file': '',\n",
       "  'default_data_types': 'Python',\n",
       "  'unique_functions': 'hash',\n",
       "  'allow_view_arguments': False,\n",
       "  'inline_sdfgs': False,\n",
       "  'max_stack_array_size': 65536,\n",
       "  'fpga': {'autobuild_bitstreams': True,\n",
       "   'minimum_fifo_depth': '',\n",
       "   'vendor': 'xilinx',\n",
       "   'concurrent_kernel_detection': False},\n",
       "  'xilinx': {'mode': 'simulation',\n",
       "   'path': '',\n",
       "   'platform': 'xilinx_u250_xdma_201830_2',\n",
       "   'frequency': '',\n",
       "   'enable_debugging': False,\n",
       "   'host_flags': '-Wno-unknown-pragmas -Wno-unused-label',\n",
       "   'synthesis_flags': '-std=c++14',\n",
       "   'build_flags': '',\n",
       "   'decouple_array_interfaces': False},\n",
       "  'intel_fpga': {'mode': 'emulator',\n",
       "   'path': '',\n",
       "   'board': 'a10gx',\n",
       "   'enable_debugging': False,\n",
       "   'host_flags': '-Wno-unknown-pragmas',\n",
       "   'kernel_flags': '-fp-relaxed -cl-no-signed-zeros -cl-fast-relaxed-math -cl-single-precision-constant -no-interleaving=default'},\n",
       "  'rtl': {'verbose': False,\n",
       "   'verilator_flags': '',\n",
       "   'verilator_lint_warnings': True,\n",
       "   'verilator_enable_debug': False},\n",
       "  'mpi': {'executable': ''}},\n",
       " 'library': {'blas': {'default_implementation': 'OpenBLAS',\n",
       "   'override': False,\n",
       "   'fpga': {'default_stream_depth': 32}},\n",
       "  'lapack': {'default_implementation': 'OpenBLAS', 'override': False},\n",
       "  'linalg': {'default_implementation': 'OpenBLAS', 'override': False},\n",
       "  'pblas': {'default_implementation': 'MKLMPICH', 'override': False}},\n",
       " 'optimizer': {'autospecialize': False,\n",
       "  'autooptimize': False,\n",
       "  'autotile_size': 128,\n",
       "  'autotile_partial_parallelism': True,\n",
       "  'visualize_sdfv': False,\n",
       "  'save_intermediate': False,\n",
       "  'automatic_simplification': True,\n",
       "  'detect_control_flow': True,\n",
       "  'symbolic_positive': True,\n",
       "  'match_exception': False},\n",
       " 'instrumentation': {'report_each_invocation': True,\n",
       "  'papi': {'default_counters': \"['PAPI_TOT_INS', 'PAPI_TOT_CYC', 'PAPI_L2_TCM', 'PAPI_L3_TCM']\",\n",
       "   'overhead_compensation': True,\n",
       "   'vectorization_analysis': False},\n",
       "  'print_fpga_runtime': False},\n",
       " 'progress': True,\n",
       " 'cache': 'name',\n",
       " 'store_history': True,\n",
       " 'default_build_folder': '.dacecache',\n",
       " 'profiling': False,\n",
       " 'profiling_status': True,\n",
       " 'treps': 100,\n",
       " 'call_hooks': '',\n",
       " 'compiled_sdfg_call_hooks': '',\n",
       " 'external_transformations_path': '$HOME/dace_transformations/external_transformations',\n",
       " 'experimental': {'validate_undefs': False, 'check_race_conditions': False},\n",
       " 'testing': {'serialization': False,\n",
       "  'deserialize_exception': False,\n",
       "  'serialize_all_fields': False}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04e791bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dace.config.Config"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9cbf749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing known BLAS/LAPACK libs under /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/lib\n",
      "  libblas*: 2\n",
      "   - libblas.3.dylib\n",
      "   - libblas.dylib\n",
      "  libcblas*: 2\n",
      "   - libcblas.3.dylib\n",
      "   - libcblas.dylib\n",
      "  liblapack*: 2\n",
      "   - liblapack.3.dylib\n",
      "   - liblapack.dylib\n",
      "  liblapacke*: 0\n",
      "  libopenblas*: 7\n",
      "   - libopenblas.0.dylib\n",
      "   - libopenblas.a\n",
      "   - libopenblas.dylib\n",
      "   - libopenblas_armv8p-r0.3.30.dylib\n",
      "   - libopenblas_vortexp-r0.3.30.a\n",
      "   ...\n",
      "  libmkl*: 0\n"
     ]
    }
   ],
   "source": [
    "print('Listing known BLAS/LAPACK libs under', lib)\n",
    "import glob\n",
    "for pat in ['libblas*','libcblas*','liblapack*','liblapacke*','libopenblas*','libmkl*']:\n",
    "    matches = sorted(glob.glob(os.path.join(lib, pat)))\n",
    "    print(f'  {pat}:', len(matches))\n",
    "    for m in matches[:5]:\n",
    "        print('   -', os.path.basename(m))\n",
    "    if len(matches) > 5:\n",
    "        print('   ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd345d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ctypes.find_library:')\n",
    "for name in ['blas','cblas','lapacke','lapack','openblas','mkl_rt']:\n",
    "    print(f'  {name}:', ctypes.util.find_library(name))\n",
    "cp = os.environ.get('CONDA_PREFIX')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76fd5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"LibraryNode base class: {dace.sdfg.nodes.LibraryNode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4916fbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['compiler', 'library', 'debugprint', 'optimizer', 'instrumentation', 'frontend', 'progress', 'cache', 'store_history', 'default_build_folder', 'profiling', 'profiling_status', 'treps', 'call_hooks', 'compiled_sdfg_call_hooks', 'external_transformations_path', 'experimental', 'testing'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_conf.keys() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152fdcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'dict',\n",
       " 'title': 'General',\n",
       " 'description': 'DaCe Preferences',\n",
       " 'required': {'optimizer': {'type': 'dict',\n",
       "   'title': 'Optimizer',\n",
       "   'description': 'Preferences of the SDFG Optimizer',\n",
       "   'required': {'autospecialize': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Auto-specialize symbols',\n",
       "     'description': 'Automatically specialize every SDFG to the symbol values at call-time. Requires all symbols to be set.\\n'},\n",
       "    'autooptimize': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Run auto-optimization heuristics',\n",
       "     'description': 'Automatically runs the set of optimizing transformation heuristics on any program called via the Python frontend.\\n'},\n",
       "    'autotile_size': {'type': 'int',\n",
       "     'default': 128,\n",
       "     'title': 'Default tile size in auto-optimization',\n",
       "     'description': 'Sets the default tile size for the optimization heuristics.\\n'},\n",
       "    'autotile_partial_parallelism': {'type': 'bool',\n",
       "     'default': True,\n",
       "     'title': 'Prefer partial parallelism over write-conflict tiling',\n",
       "     'description': 'If true, sets the auto-optimizer to prefer extracting map parallel dimensions over tiling for atomic write-conflict resolution edges. This may be slower in case of small parallel dimensions vs. conflicted dimensions. This preference only applies to symbolic ranges or ranges over the autotile_size parameter.\\n'},\n",
       "    'visualize_sdfv': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Visualize SDFG',\n",
       "     'description': 'Open an SDFG in browser every transformation.'},\n",
       "    'save_intermediate': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Save intermediate SDFGs',\n",
       "     'description': 'Save SDFG files after every transformation.'},\n",
       "    'automatic_simplification': {'type': 'bool',\n",
       "     'default': True,\n",
       "     'title': 'Automatic SDFG simplification',\n",
       "     'description': 'Automatically performs SDFG simplification on programs.\\n'},\n",
       "    'detect_control_flow': {'type': 'bool',\n",
       "     'default': True,\n",
       "     'title': 'Detect control flow from state transitions',\n",
       "     'description': 'Attempts to infer control flow constructs \"if\", \"for\" and \"while\" from state transitions, allowing code generators to generate appropriate code.\\n'},\n",
       "    'symbolic_positive': {'type': 'bool',\n",
       "     'default': True,\n",
       "     'title': 'Treat all symbolic expressions as positive',\n",
       "     'description': 'Every expression in which a symbolic value appears is treated as strictly positive. This is necessary for certain Range evaluations using Subgraph Fusion.\\n'},\n",
       "    'match_exception': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Treat exceptions in \"can_be_applied\" as errors',\n",
       "     'description': 'When an exception is raised in a transformation \"can_be_applied\" function, if True the exception is raised further. Otherwise the exception is printed as a warning.\\n'}}},\n",
       "  'compiler': {'type': 'dict',\n",
       "   'title': 'Compiler',\n",
       "   'description': 'Preferences of the compiler',\n",
       "   'required': {'use_cache': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Use cache',\n",
       "     'description': 'If enabled, does not recompile code generated from SDFGs if shared library (.so/.dll) file is present.\\n'},\n",
       "    'library_prefix': {'type': 'str',\n",
       "     'default': '',\n",
       "     'default_Linux': 'lib',\n",
       "     'default_Darwin': 'lib',\n",
       "     'title': 'Library prefix',\n",
       "     'description': 'Filename prefix for shared libraries.'},\n",
       "    'library_extension': {'type': 'str',\n",
       "     'default': 'so',\n",
       "     'default_Linux': 'so',\n",
       "     'default_Windows': 'dll',\n",
       "     'default_Darwin': 'dylib',\n",
       "     'title': 'Library extension',\n",
       "     'description': 'File extension of shared libraries.'},\n",
       "    'indentation_spaces': {'type': 'int',\n",
       "     'default': 4,\n",
       "     'title': 'Indentation width',\n",
       "     'description': 'Number of spaces used when indenting generated code.\\n'},\n",
       "    'build_type': {'type': 'str',\n",
       "     'default': 'RelWithDebInfo',\n",
       "     'title': 'Build configuration',\n",
       "     'description': 'Configuration type for CMake build (can be Debug, Release, RelWithDebInfo, or MinSizeRel).\\n'},\n",
       "    'allow_shadowing': {'type': 'bool',\n",
       "     'default': True,\n",
       "     'title': 'Allow variable shadowing',\n",
       "     'description': 'Allowing shadowing of variables in the code (reduces exceptions to warnings when shadowing is encountered).\\n'},\n",
       "    'codegen_lineinfo': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Annotate code generator lines',\n",
       "     'description': 'Keep a source mapping between generated code and the file/line of the code generator that generated it. Used for debugging code generation.\\n'},\n",
       "    'codegen_state_struct_suffix': {'type': 'str',\n",
       "     'default': '_state_t',\n",
       "     'title': 'Suffix used by the code generator to mangle the state struct.',\n",
       "     'description': \"For every SDFG the code generator is is processing a state struct is generated. The typename of this struct is derived by appending this value to the SDFG's name. Note that the suffix may only contains letters, digits and underscores.\\n\"},\n",
       "    'format_code': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Format code with `clang-format`',\n",
       "     'description': 'Formats the generated code with `clang-format` before saving the files.\\n'},\n",
       "    'format_config_file': {'type': 'str',\n",
       "     'default': '',\n",
       "     'title': 'Path the clang-format file',\n",
       "     'description': 'Clang-format file to be used by clang-format, only used if format_code is true\\n'},\n",
       "    'default_data_types': {'type': 'str',\n",
       "     'default': 'Python',\n",
       "     'title': 'Default data types',\n",
       "     'description': 'Specify the default data types to use in generating code. If \"Python\", Python\\'s semantics will be followed (i.e., `float`  and `int` are represented using 64 bits). If the property is set to \"C\", C\\'s semantcs will be used (`float` and `int` are represented using 32bits).\\n'},\n",
       "    'unique_functions': {'type': 'str',\n",
       "     'default': 'hash',\n",
       "     'title': 'Generate unique functions',\n",
       "     'description': 'Determine if and how to generate the code for equivalent NestedSDFGs: \"hash\": hashing is used to determine if multiple NestedSDFGs with equivalent contents exist. If this is the case, the code is generated only once. \"unique_name\": the unique_name property of SDFG is used to determine if two NestedSDFGs are equal, generating the code only once.  This gives more control to the programmer, that can explicitly decide what NestedSDFG code can be replicated and what not. \"none\": a separate function is code generated for each NestedSDFG\\n'},\n",
       "    'allow_view_arguments': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Allow numpy views as arguments',\n",
       "     'description': 'If true, allows users to call DaCe programs with NumPy views (for example, \"A[:, 1]\" or \"w.T\"). As this can create pointer aliasing issues with two arrays pointing to the same memory, or analyzability issue with strides and alignment, this option is disabled by default.\\n'},\n",
       "    'inline_sdfgs': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Inline all nested SDFGs',\n",
       "     'description': 'If set to true, inlines all nested SDFGs upon code generation by default.\\n'},\n",
       "    'max_stack_array_size': {'type': 'int',\n",
       "     'default': 65536,\n",
       "     'title': 'Max stack-allocated array size (bytes)',\n",
       "     'description': 'All stack allocated arrays (i.e. StorageType.Register) with size larger than this will be allocated on the heap.\\n'},\n",
       "    'extra_cmake_args': {'type': 'str',\n",
       "     'default': '',\n",
       "     'title': 'Additional CMake configuration arguments',\n",
       "     'description': 'If set, specifies additional arguments to the initial invocation of ``cmake``.\\n'},\n",
       "    'cpu': {'type': 'dict',\n",
       "     'title': 'CPU',\n",
       "     'description': 'CPU compiler preferences',\n",
       "     'required': {'executable': {'type': 'str',\n",
       "       'default': '',\n",
       "       'title': 'Compiler executable override',\n",
       "       'description': 'File path or name of compiler executable'},\n",
       "      'args': {'type': 'str',\n",
       "       'title': 'Arguments',\n",
       "       'description': 'Compiler argument flags',\n",
       "       'default': '-std=c++14 -fPIC -Wall -Wextra -O3 -march=native -ffast-math -Wno-unused-parameter -Wno-unused-label',\n",
       "       'default_Windows': '/O2 /fp:fast /arch:AVX2 /D_USRDLL /D_WINDLL /D__restrict__=__restrict'},\n",
       "      'libs': {'type': 'str',\n",
       "       'title': 'Additional libraries',\n",
       "       'description': 'Additional linked libraries required by target',\n",
       "       'default': ''},\n",
       "      'openmp_sections': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Use OpenMP sections',\n",
       "       'description': 'If set to true, multiple connected components will generate \"#pragma omp parallel sections\" code around them.\\n'}}},\n",
       "    'cuda': {'type': 'dict',\n",
       "     'title': 'GPU',\n",
       "     'description': 'GPU (CUDA/HIP) compiler preferences',\n",
       "     'required': {'backend': {'type': 'str',\n",
       "       'default': 'auto',\n",
       "       'title': 'Compilation backend',\n",
       "       'description': \"Backend to compile for ('auto' for automatic detection, 'cuda' for NVIDIA, or 'hip' for AMD).\\n\"},\n",
       "      'path': {'type': 'str',\n",
       "       'default': '',\n",
       "       'title': 'CUDA/HIP path override',\n",
       "       'description': 'Path to CUDA toolkit or ROCm/HIP root directory'},\n",
       "      'args': {'type': 'str',\n",
       "       'title': 'nvcc Arguments',\n",
       "       'description': 'Compiler argument flags for CUDA',\n",
       "       'default': '-Xcompiler -march=native --use_fast_math -Xcompiler -Wno-unused-parameter',\n",
       "       'default_Windows': '-O3 --use_fast_math'},\n",
       "      'hip_args': {'type': 'str',\n",
       "       'title': 'hipcc Arguments',\n",
       "       'description': 'Compiler argument flags for HIP',\n",
       "       'default': '-std=c++17 -fPIC -O3 -ffast-math -Wno-unused-parameter'},\n",
       "      'cuda_arch': {'type': 'str',\n",
       "       'title': 'Additional CUDA architectures',\n",
       "       'description': 'Additional CUDA architectures (separated by commas) to compile GPU code for, excluding the current architecture on the compiling machine.\\n',\n",
       "       'default': '60'},\n",
       "      'hip_arch': {'type': 'str',\n",
       "       'title': 'Additional HIP architectures',\n",
       "       'description': 'Additional HIP architectures (separated by commas) to compile GPU code for, excluding the current architecture on the compiling machine.\\n',\n",
       "       'default': 'gfx906'},\n",
       "      'default_block_size': {'type': 'str',\n",
       "       'title': 'Default thread-block size',\n",
       "       'description': \"Default thread-block size for GPU kernels when explicit GPU block maps are not defined. Can be set to 'max' to maximize occupancy.\\n\",\n",
       "       'default': '32,1,1'},\n",
       "      'dynamic_map_block_size': {'type': 'str',\n",
       "       'title': 'Thread-Block size for GPU_ThreadBlock_Dynamic',\n",
       "       'description': \"Thread-Block size for maps using GPU_ThreadBlock_Dynamic scheduler. Can be set to 'max' to maximize occupancy.\\n\",\n",
       "       'default': '128,1,1'},\n",
       "      'dynamic_map_fine_grained': {'type': 'bool',\n",
       "       'title': 'Enable fine grained load balancing for GPU_ThreadBlock_Dynamic',\n",
       "       'description': 'If true the scheduler will dynamically redistribute the combined work of all threads in the warp equally across the warp (fine grained). Otherwise, each warp works sequentially only on its tasks (potential load imbalance).\\n',\n",
       "       'default': True},\n",
       "      'persistent_map_SM_fraction': {'type': 'float',\n",
       "       'title': 'Fraction of SMs to use for persistent GPU map',\n",
       "       'description': 'Sets the fraction of the number of SMs of the Device that the GPU_Persistent map can use. Together with persistent_map_occupancy this specifies the grid size of the kernel being launched. 0.0 < persistent_map_SM_fraction <= 1.0 The fraction will be rounded up to the next integer number of SMs. The max value of SMs that can/will be used is equal to cudaDevAttrMultiProcessorCount.\\n',\n",
       "       'default': 1.0},\n",
       "      'persistent_map_occupancy': {'type': 'int',\n",
       "       'title': 'Number of blocks to launch per SM used',\n",
       "       'description': 'Sets the number of thread block to be launched per SM being used. Essentially this is a simple multiplier to persistent_map_SM_fraction. It is up to the user to check if the resulting number of thread blocks can run efficiently on the GPU.\\n',\n",
       "       'default': 2},\n",
       "      'max_concurrent_streams': {'type': 'int',\n",
       "       'title': 'Concurrent execution streams',\n",
       "       'description': 'Maximum number of concurrent CUDA/HIP streams to generate. Special values: -1 only uses the default stream, 0 uses infinite concurrent streams.\\n',\n",
       "       'default': 0},\n",
       "      'syncdebug': {'type': 'bool',\n",
       "       'title': 'Synchronous Debugging',\n",
       "       'description': 'Enables Synchronous Debugging mode, where each library call is followed by full-device synchronization and error checking.\\n',\n",
       "       'default': False},\n",
       "      'libs': {'type': 'str',\n",
       "       'title': 'Additional libraries',\n",
       "       'description': 'Additional linked libraries required by target',\n",
       "       'default': ''},\n",
       "      'mempool_release_threshold': {'type': 'int',\n",
       "       'title': 'Memory pool memory release threshold',\n",
       "       'default': -1,\n",
       "       'description': 'A value that determines how large a memory allocation has to be before it is automatically released from the memory pool to the system. The default is -1, which indicates \"never release\". Other values may be 0 (always release), or any byte value. For more information, see ``cudaMemPoolAttrReleaseThreshold`` in the CUDA toolkit documentation.\\n'},\n",
       "      'block_size_limit': {'type': 'int',\n",
       "       'title': 'Maximum thread-block size in code generation',\n",
       "       'default': 1024,\n",
       "       'description': 'Threshold for the GPU code generator to fail in generating a kernel with a specified overall larger block size. Default value is derived from hardware limits on common GPUs.\\n'},\n",
       "      'block_size_lastdim_limit': {'type': 'int',\n",
       "       'title': 'Maximum last dimension thread-block size in code generation',\n",
       "       'default': 64,\n",
       "       'description': 'Threshold for the GPU code generator to fail in generating a kernel with a specified larger block size in the third dimension. Default value is derived from hardware limits on common GPUs.\\n'},\n",
       "      'thread_id_type': {'type': 'str',\n",
       "       'title': 'Thread/block index data type',\n",
       "       'default': 'int32',\n",
       "       'description': 'Defines the data type for a thread and block index in the generated code. The type is based on the type-classes in ``dace.dtypes``. For example, ``uint64`` is equivalent to ``dace.uint64``. Change this setting when large index types are needed to address memory offsets that are beyond the 32-bit range, or to reduce memory usage.\\n'},\n",
       "      'allow_implicit_memlet_to_map': {'type': 'bool',\n",
       "       'title': 'Allow the implicit conversion of Memlets to Maps during code generation.',\n",
       "       'default': True,\n",
       "       'description': 'If ``true`` the code generator will implicitly convert Memlets that cannot be represented by a native library call, such as ``cudaMemcpy()`` into Maps that explicitly copy the data around. If this value is ``false`` the code generator will raise an exception if such a Memlet is encountered. This allows the user to have full control over all Maps in the SDFG.\\n'}}},\n",
       "    'fpga': {'type': 'dict',\n",
       "     'title': 'FPGA',\n",
       "     'description': 'Common preferences for FPGA compilation.',\n",
       "     'required': {'autobuild_bitstreams': {'type': 'bool',\n",
       "       'default': True,\n",
       "       'title': 'Automatically build bitstreams',\n",
       "       'description': 'If set to true, CMake will automatically build missing bitstreams when running an FPGA program. This can take a very long time, and users might want to do this manually. If set to false, the program will optimistically assume that the bitstream is present in the build directory, and will crash if this is not the case.\\n'},\n",
       "      'minimum_fifo_depth': {'type': 'int',\n",
       "       'default': '',\n",
       "       'title': 'Minimum depth of FIFOs',\n",
       "       'description': 'Sets the minimum depth of any generated FIFO.'},\n",
       "      'vendor': {'type': 'str',\n",
       "       'default': 'xilinx',\n",
       "       'title': 'FPGA vendor',\n",
       "       'description': 'Target Xilinx (\"xilinx\") or Intel (\"intel_fpga\") FPGAs when generating code.\\n'},\n",
       "      'concurrent_kernel_detection': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Detect parts of an SDFG that can run in parallel',\n",
       "       'description': 'If set to false, DaCe will place each weakly connected component found in an SDFG state in a different Kernel/Processing Element. If true, a heuristic will further inspect each independent component for other parallelism opportunities (e.g., branches of the SDFG that can be executed in parallel), creating the corresponding kernels.\\n'}}},\n",
       "    'xilinx': {'type': 'dict',\n",
       "     'title': 'Xilinx',\n",
       "     'description': 'FPGA (Xilinx) compiler preferences',\n",
       "     'required': {'mode': {'type': 'str',\n",
       "       'default': 'simulation',\n",
       "       'title': 'Compilation mode',\n",
       "       'description': 'Target of FPGA kernel build (simulation/software_emulation/hardware_emulation/hardware)'},\n",
       "      'path': {'type': 'str',\n",
       "       'default': '',\n",
       "       'title': 'Vitis installation override',\n",
       "       'description': 'Path to specific Vitis/SDx/SDAccel installation to use instead of just searching PATH and environment variables.\\n'},\n",
       "      'platform': {'type': 'str',\n",
       "       'default': 'xilinx_u250_xdma_201830_2',\n",
       "       'title': 'Target platform for Xilinx',\n",
       "       'description': 'Platform name of Vitis/SDx/SDAccel target.'},\n",
       "      'frequency': {'type': 'str',\n",
       "       'default': '',\n",
       "       'title': 'Target frequency for Xilinx kernels',\n",
       "       'description': 'Target frequency, in MHz, when compiling kernels for Xilinx. Will not necessarily be achieved in practice. To enable multiple clocks, enter values in the format \"clock_id:frequency\", with frequency being specified in MHz separated by an escaped bar, all enclosed in quotes. E.g. \"0:250\\\\|1:500\".\\n'},\n",
       "      'enable_debugging': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Enable debugging for hardware kernels',\n",
       "       'description': 'Injects debugging cores on the interfaces of the kernel, allowing fine-grained debugging of hardware runs at the cost of additional resources. This is always enabled for emulation runs.\\n'},\n",
       "      'host_flags': {'type': 'str',\n",
       "       'title': 'Host arguments',\n",
       "       'description': 'Extra host compiler argument flags',\n",
       "       'default': '-Wno-unknown-pragmas -Wno-unused-label'},\n",
       "      'synthesis_flags': {'type': 'str',\n",
       "       'title': 'Synthesis arguments',\n",
       "       'description': 'High-level synthesis C++ flags',\n",
       "       'default': '-std=c++14'},\n",
       "      'build_flags': {'type': 'str',\n",
       "       'title': 'Arguments',\n",
       "       'description': 'Kernel build C++ flags',\n",
       "       'default': ''},\n",
       "      'decouple_array_interfaces': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Decouple array memory interfaces',\n",
       "       'description': 'If an array is both read and written, this option decouples its accesses, by creatin a memory interface for reading and one for writing. Note that this may hide potential Read-After-Write or Write-After-Read dependencies.\\n'}}},\n",
       "    'intel_fpga': {'type': 'dict',\n",
       "     'title': 'Intel FPGA',\n",
       "     'description': 'Intel FPGA compiler preferences.',\n",
       "     'required': {'mode': {'type': 'str',\n",
       "       'default': 'emulator',\n",
       "       'title': 'Compilation mode',\n",
       "       'description': 'Target of FPGA kernel build (emulator/simulator/hardware).\\n'},\n",
       "      'path': {'type': 'str',\n",
       "       'default': '',\n",
       "       'title': 'Intel FPGA OpenCL SDK installation override',\n",
       "       'description': 'Path to specific Intel FPGA OpenCL SDK installation to use instead of just searching PATH and environment variables.\\n'},\n",
       "      'board': {'type': 'str',\n",
       "       'default': 'a10gx',\n",
       "       'title': 'Target FPGA board',\n",
       "       'description': 'FPGA board to compile for, obtain list by running ``aoc --list-boards``.'},\n",
       "      'enable_debugging': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Enable debugging for hardware kernels',\n",
       "       'description': 'Injects debugging cores where available.'},\n",
       "      'host_flags': {'type': 'str',\n",
       "       'title': 'Host arguments',\n",
       "       'description': 'Extra host compiler argument flags',\n",
       "       'default': '-Wno-unknown-pragmas'},\n",
       "      'kernel_flags': {'type': 'str',\n",
       "       'title': 'Kernel flags',\n",
       "       'description': 'High-level synthesis C++ flags',\n",
       "       'default': '-fp-relaxed -cl-no-signed-zeros -cl-fast-relaxed-math -cl-single-precision-constant -no-interleaving=default'}}},\n",
       "    'rtl': {'type': 'dict',\n",
       "     'title': 'RTL',\n",
       "     'description': 'RTL (SystemVerilog) compiler preferences',\n",
       "     'required': {'verbose': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Verbose Build & Execution Output',\n",
       "       'description': 'Output full build and execution (incl internal state) log.'},\n",
       "      'verilator_flags': {'type': 'str',\n",
       "       'default': '',\n",
       "       'title': 'Additional Verilator Arguments',\n",
       "       'description': 'Additional arguments feed to verilator.'},\n",
       "      'verilator_lint_warnings': {'type': 'bool',\n",
       "       'default': True,\n",
       "       'title': 'Verilator Lint Warnings',\n",
       "       'description': 'Enable/Disable detailed SV lint checker output.'},\n",
       "      'verilator_enable_debug': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Verilator Enable Debug',\n",
       "       'description': 'Enable/disable verbose internal state debug output.'}}},\n",
       "    'mpi': {'type': 'dict',\n",
       "     'title': 'MPI',\n",
       "     'description': 'MPI compiler preferences',\n",
       "     'required': {'executable': {'type': 'str',\n",
       "       'default': '',\n",
       "       'title': 'Compiler executable override',\n",
       "       'description': 'File path or name of compiler executable'}}},\n",
       "    'linker': {'type': 'dict',\n",
       "     'title': 'Linker',\n",
       "     'description': 'Linker preferences',\n",
       "     'required': {'executable': {'type': 'str',\n",
       "       'default': '',\n",
       "       'title': 'Linker executable override',\n",
       "       'description': 'File path or name of linker executable'},\n",
       "      'args': {'type': 'str',\n",
       "       'title': 'Arguments',\n",
       "       'description': 'Linker argument flags',\n",
       "       'default': '-Wl,--disable-new-dtags',\n",
       "       'default_Darwin': '',\n",
       "       'default_Windows': ''}}}}},\n",
       "  'instrumentation': {'type': 'dict',\n",
       "   'title': 'Instrumentation',\n",
       "   'description': 'Instrumentation preferences',\n",
       "   'required': {'report_each_invocation': {'type': 'bool',\n",
       "     'title': 'Save report for each invocation',\n",
       "     'default': True,\n",
       "     'description': 'Save an instrumentation report file for each invocation of the SDFG, rather than one report that spans from SDFG initialization to finalization.\\n'},\n",
       "    'papi': {'type': 'dict',\n",
       "     'title': 'PAPI',\n",
       "     'description': 'PAPI configuration',\n",
       "     'required': {'default_counters': {'type': 'str',\n",
       "       'title': 'Default PAPI counters',\n",
       "       'default': \"['PAPI_TOT_INS', 'PAPI_TOT_CYC', 'PAPI_L2_TCM', 'PAPI_L3_TCM']\",\n",
       "       'description': 'Sets the default PAPI counter list, formatted as a Python list of strings.\\n'},\n",
       "      'overhead_compensation': {'type': 'bool',\n",
       "       'title': 'Compensate Overhead',\n",
       "       'default': True,\n",
       "       'description': 'Subtracts the minimum measured overhead from every measurement.\\n'},\n",
       "      'vectorization_analysis': {'type': 'bool',\n",
       "       'title': 'Enable vectorization check',\n",
       "       'default': False,\n",
       "       'description': 'Enables analysis of gcc vectorization information. Only gcc/g++ is supported.\\n'}}},\n",
       "    'print_fpga_runtime': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Print FPGA runtime',\n",
       "     'description': 'Prints the runtime of instrumented FPGA kernel states to standard output.'}}},\n",
       "  'frontend': {'type': 'dict',\n",
       "   'title': 'Frontend',\n",
       "   'description': 'Python frontend preferences',\n",
       "   'required': {'cache_size': {'type': 'int',\n",
       "     'title': 'Program cache size',\n",
       "     'default': 32,\n",
       "     'description': 'The number of compiled programs to cache (based on argument types, closure constants, and closure array types) to avoid reparsing/compiling when calling a @dace.program or method.\\n'},\n",
       "    'implicit_recursion_depth': {'type': 'int',\n",
       "     'title': 'Auto-parsing recursion depth',\n",
       "     'default': 64,\n",
       "     'description': 'The maximum call-stack depth allowed when automatically parsing called dace functions or methods.\\n'},\n",
       "    'raise_nested_parsing_errors': {'type': 'bool',\n",
       "     'title': 'Raise nested parsing errors',\n",
       "     'default': False,\n",
       "     'description': 'Raise all errors out of nested function parsing contexts instead of trying to create a callback implicitly.\\n'},\n",
       "    'verbose_errors': {'type': 'bool',\n",
       "     'title': 'Show preprocessed AST on parsing errors',\n",
       "     'default': False,\n",
       "     'description': 'Prints out the preprocessed unparsed AST in case of a parsing error.\\n'},\n",
       "    'preprocessing_passes': {'type': 'int',\n",
       "     'title': 'Number of preprocessing passes on Python code',\n",
       "     'default': 5,\n",
       "     'description': 'Number of times to run the Python preprocessing passes (e.g., constant folding) on the input code. Set to zero to disable preprocessing optimizations, set to -1 to run until the code has not changed.\\n'},\n",
       "    'dont_fuse_callbacks': {'type': 'bool',\n",
       "     'title': 'Do not fuse callbacks',\n",
       "     'default': False,\n",
       "     'description': \"Stricter mode of operation where callbacks into Python don't participate in state fusion transformations.\\n\"},\n",
       "    'typed_callbacks_only': {'type': 'bool',\n",
       "     'title': 'Only allow typed callbacks',\n",
       "     'default': False,\n",
       "     'description': 'Stricter mode of operation where callbacks into Python must have explicit return value types in order to compile.\\n'},\n",
       "    'unroll_threshold': {'type': 'int',\n",
       "     'title': 'Automatic unroll loop size threshold',\n",
       "     'default': -1,\n",
       "     'description': 'Threshold for automatic loop unrolling of any generator (e.g., including ``range``) with a compile-time size. A value of -1 (default) means not to unroll any loop automatically, a value of 0 means unrolling every loop, and a value above zero sets a size threshold beyond which a constant-sized loop will not be automatically unrolled.\\n'},\n",
       "    'check_args': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Check arguments on SDFG call',\n",
       "     'description': 'Perform an early type check on arguments passed to an SDFG when called directly (from ``SDFG.__call__``). Another type check is performed when calling compiled SDFGs.\\n'},\n",
       "    'avoid_wcr': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Avoid using WCR for augmented assignments when possible',\n",
       "     'description': 'Perform a map-symbol-dependency check on the write-subsets of augmented assignments that appear inside Maps to avoid using WCR when possible. This feature works correctly only when there is a single augmented assignment for each data dimension inside a Map.\\n'}}},\n",
       "  'debugprint': {'type': 'bool',\n",
       "   'default': False,\n",
       "   'title': 'Debug printing',\n",
       "   'description': 'Enable verbose printouts.'},\n",
       "  'progress': {'type': 'bool',\n",
       "   'default': True,\n",
       "   'title': 'Progress reports',\n",
       "   'description': 'Enable progress report printouts.'},\n",
       "  'cache': {'type': 'str',\n",
       "   'default': 'name',\n",
       "   'title': 'Compiled cache entry naming policy',\n",
       "   'description': 'Determine the name of the generated ``.dacecache`` folder:\\n\\n* ``name`` uses the name of the SDFG directly, causing it to be overridden by other programs using the same SDFG name.\\n* ``hash`` uses a mangled name based on the hash of the SDFG, such that any change to the SDFG will generate a different cache folder.\\n* ``unique`` uses a name based on the currently running Python process at code generation time, such that no caching or clashes can happen between different processes or subsequent invocations of Python.\\n* ``single`` uses a single cache folder for all SDFGs, saving space and potentially build time, but disallows executing SDFGs in parallel and caching of more than one simultaneous SDFG.\\n'},\n",
       "  'store_history': {'type': 'bool',\n",
       "   'default': True,\n",
       "   'title': 'Store SDFG transformation history',\n",
       "   'description': 'Store the history of transformations on the SDFG file.'},\n",
       "  'default_build_folder': {'type': 'str',\n",
       "   'default': '.dacecache',\n",
       "   'title': 'Default SDFG build folder',\n",
       "   'description': 'Default folder in which compiled DaCe programs and SDFGs are stored. Can either be a relative path (by default) or absolute.\\n'},\n",
       "  'profiling': {'type': 'bool',\n",
       "   'default': False,\n",
       "   'title': 'Profiling',\n",
       "   'description': 'Enable profiling support.'},\n",
       "  'profiling_status': {'type': 'bool',\n",
       "   'default': True,\n",
       "   'title': 'Status bar for profiling',\n",
       "   'description': 'Enable tqdm status bar while profiling. If tqdm is not installed a warning will appear. To disable this feature (and the warning) set this option to false.\\n'},\n",
       "  'treps': {'type': 'int',\n",
       "   'default': 100,\n",
       "   'title': 'Profiling Repetitions',\n",
       "   'description': 'Number of times to run program for profiling.'},\n",
       "  'call_hooks': {'type': 'str',\n",
       "   'default': '',\n",
       "   'title': 'Hooks before/after every DaCe program call',\n",
       "   'description': 'A comma-separated list of functions (or Context Manager classes) that will be called before every DaCe program (SDFG) is compiled and run. Used for functionality such as automatic tuning or instrumentation.\\n'},\n",
       "  'compiled_sdfg_call_hooks': {'type': 'str',\n",
       "   'default': '',\n",
       "   'title': 'Hooks before/after every compiled SDFG call',\n",
       "   'description': \"A comma-separated list of functions (or Context Manager classes) that will be called before every compiled SDFG's generated code is invoked. Used for functionality such as low-level profiling.\\n\"},\n",
       "  'external_transformations_path': {'type': 'str',\n",
       "   'default': '$HOME/dace_transformations/external_transformations',\n",
       "   'default_Windows': '%USERPROFILE%\\\\\\\\dace_transformations\\\\\\\\external_transformations',\n",
       "   'title': 'External transformations path',\n",
       "   'description': 'Path to a directory containing external transformations that are not included in the main DaCe package. This path is added to the Python path and can be used to import custom transformation modules.\\n'},\n",
       "  'experimental': {'type': 'dict',\n",
       "   'title': 'Experimental',\n",
       "   'description': 'Experimental features',\n",
       "   'required': {'validate_undefs': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Undefined Symbol Check',\n",
       "     'description': 'Check for undefined symbols in memlets during SDFG validation.\\n'},\n",
       "    'check_race_conditions': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Check race conditions',\n",
       "     'description': 'Check for potential race conditions during validation.\\n'}}},\n",
       "  'testing': {'type': 'dict',\n",
       "   'title': 'Testing',\n",
       "   'description': 'Unit testing settings',\n",
       "   'required': {'serialization': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Test Serialization on validation',\n",
       "     'description': 'Before generating code, verify that a serialization/deserialization loop generates the same SDFG.\\n'},\n",
       "    'deserialize_exception': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Treat exceptions in deserialization as errors',\n",
       "     'description': 'When an exception is raised in a deserialization process (e.g., due to missing library node), by default a warning is issued. If this setting is True, the exception will be raised as-is.\\n'},\n",
       "    'serialize_all_fields': {'type': 'bool',\n",
       "     'default': False,\n",
       "     'title': 'Serialize all unmodified fields in SDFG files',\n",
       "     'description': 'If False (default), saving an SDFG keeps only the modified non-default properties. If True, saves all fields.\\n'}}},\n",
       "  'library': {'type': 'dict',\n",
       "   'title': 'Library',\n",
       "   'description': 'Settings for handling the use of DaCe libraries.',\n",
       "   'required': {'blas': {'type': 'dict',\n",
       "     'title': 'BLAS',\n",
       "     'description': 'Built-in BLAS DaCe library.',\n",
       "     'required': {'default_implementation': {'type': 'str',\n",
       "       'default': 'pure',\n",
       "       'title': 'Default implementation',\n",
       "       'description': 'Default implementation for BLAS library nodes.'},\n",
       "      'override': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Force configured implementation',\n",
       "       'description': 'Force the default implementation, even if an implementation has been explicitly set on a node.\\n'},\n",
       "      'fpga': {'type': 'dict',\n",
       "       'title': 'FPGA',\n",
       "       'description': 'FPGA-specific BLAS options.',\n",
       "       'required': {'default_stream_depth': {'type': 'int',\n",
       "         'default': 32,\n",
       "         'title': 'Default FPGA stream depth',\n",
       "         'description': 'Default FPGA stream depth used in the BLAS library nodes and the corresponding streaming transformations\\n'}}}}},\n",
       "    'lapack': {'type': 'dict',\n",
       "     'title': 'LAPACK',\n",
       "     'description': 'Built-in LAPACK DaCe library.',\n",
       "     'required': {'default_implementation': {'type': 'str',\n",
       "       'default': 'OpenBLAS',\n",
       "       'title': 'Default implementation',\n",
       "       'description': 'Default implementation for LAPACK library nodes.'},\n",
       "      'override': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Force configured implementation',\n",
       "       'description': 'Force the default implementation, even if an implementation has been explicitly set on a node.\\n'}}},\n",
       "    'linalg': {'type': 'dict',\n",
       "     'title': 'linalg',\n",
       "     'description': 'Built-in NumPy linalg DaCe library.',\n",
       "     'required': {'default_implementation': {'type': 'str',\n",
       "       'default': 'OpenBLAS',\n",
       "       'title': 'Default implementation',\n",
       "       'description': 'Default implementation for linalg library nodes.'},\n",
       "      'override': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Force configured implementation',\n",
       "       'description': 'Force the default implementation, even if an implementation has been explicitly set on a node.\\n'}}},\n",
       "    'pblas': {'type': 'dict',\n",
       "     'title': 'PBLAS',\n",
       "     'description': 'Built-in PBLAS DaCe library.',\n",
       "     'required': {'default_implementation': {'type': 'str',\n",
       "       'default': 'MKLMPICH',\n",
       "       'title': 'Default implementation',\n",
       "       'description': 'Default implementation PBLAS library nodes.'},\n",
       "      'override': {'type': 'bool',\n",
       "       'default': False,\n",
       "       'title': 'Force configured implementation',\n",
       "       'description': 'Force the default implementation, even if an implementation has been explicitly set on a node.\\n'}}}}}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config._config_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26597cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Compiler Configuration ===\n",
      "Build type: RelWithDebInfo\n",
      "CPU compiler: /opt/homebrew/opt/llvm/bin/clang++\n",
      "Default compiler flags: -std=c++14 -fPIC -Wall -Wextra -O3 -march=native -ffast-math -Wno-unused-parameter -Wno-unused-label\n",
      "\n",
      "=== GPU Configuration ===\n",
      "GPU backend: auto\n",
      "Default block sizes: [64, 8, 1] -> block_ndim = 2\n",
      "Max concurrent streams: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print compiler configuration\n",
    "print(\"\\n=== Compiler Configuration ===\")\n",
    "print(f\"Build type: {Config.get('compiler', 'build_type')}\")\n",
    "print(f\"CPU compiler: {Config.get('compiler', 'cpu', 'executable')}\")\n",
    "print(f\"Default compiler flags: {Config.get('compiler', 'cpu', 'args')}\")\n",
    "\n",
    "# Print CUDA/GPU configuration if available\n",
    "if Config.get('compiler', 'cuda', 'backend') is not None:\n",
    "    print(\"\\n=== GPU Configuration ===\")\n",
    "    print(f\"GPU backend: {Config.get('compiler', 'cuda', 'backend')}\")\n",
    "    # print(f\"CUDA compiler: {Config.get('compiler', 'cuda', 'executable')}\")\n",
    "    # print(f\"Default CUDA flags: {Config.get('compiler', 'cuda', 'args')}\")\n",
    "    default_block_sizes = [\n",
    "        int(b) for b in Config.get('compiler', 'cuda', 'default_block_size').split(',')\n",
    "    ]\n",
    "\n",
    "    default_block_ndim = max(1, sum(1 if b != 1 else 0 for b in default_block_sizes))\n",
    "    print(f\"Default block sizes: {default_block_sizes} -> block_ndim = {default_block_ndim}\")\n",
    "    max_streams = int(Config.get('compiler', 'cuda', 'max_concurrent_streams'))\n",
    "    print(f\"Max concurrent streams: {max_streams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee0bd05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_size: [128, 1, 1]\n",
      "block_ndim: 1\n"
     ]
    }
   ],
   "source": [
    "Config.get('compiler', 'cuda', 'dynamic_map_block_size')\n",
    "block_size = [\n",
    "        int(b) for b in Config.get('compiler', 'cuda', 'dynamic_map_block_size').split(',')\n",
    "    ]\n",
    "print(f\"block_size: {block_size}\")\n",
    "block_ndim = max(1, sum(1 if b != 1 else 0 for b in block_size))\n",
    "    # grid_ndim = max(1, sum(1 if g != 1 else 0 for g in grid_size))\n",
    "print(f\"block_ndim: {block_ndim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ac5a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synchronous debugging enabled: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Synchronous debugging enabled:', Config.get_bool('compiler', 'cuda', 'syncdebug'))\n",
    "\n",
    "Config.set('frontend', 'unroll_threshold', value=11)\n",
    "Config.set('debugprint', value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e922dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Debug Configuration ===\n",
      "Debug print enabled: True\n",
      "Cache enabled: False\n",
      "Frontend verbose errors: False\n"
     ]
    }
   ],
   "source": [
    "# Print debugging configuration\n",
    "print(\"\\n=== Debug Configuration ===\")\n",
    "print(f\"Debug print enabled: {dace.config.Config.get_bool('debugprint')}\")\n",
    "print(f\"Cache enabled: {dace.config.Config.get_bool('compiler', 'use_cache')}\")\n",
    "print(f\"Frontend verbose errors: {dace.config.Config.get_bool('frontend', 'verbose_errors')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0f3e7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " python\n"
     ]
    }
   ],
   "source": [
    "config_data_types = Config.get('compiler', 'default_data_types')\n",
    "print(f\" {config_data_types.lower()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743c453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples: canonicalization via create_datadescriptor\n",
    "import dace, numpy as np\n",
    "from dace import data, dtypes, is_array, is_gpu_array\n",
    "import textwrap\n",
    "\n",
    "def h(title):\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(f\" {title}\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "def kv(name, val):\n",
    "    print(f\" - {name:<22}: {val}\")\n",
    "\n",
    "def show_typeclass(tc, label=None):\n",
    "    label = label or str(tc)\n",
    "    try:\n",
    "        npdtype = tc.as_numpy_dtype()\n",
    "    except Exception:\n",
    "        npdtype = \"-\"\n",
    "    try:\n",
    "        ctypes = tc.as_ctypes()\n",
    "    except Exception:\n",
    "        ctypes = \"-\"\n",
    "    print(f\"{label:<22}  numpy: {npdtype!s:<12}  ctypes: {ctypes!s}\")\n",
    "def show_interfaces(x):\n",
    "    print(f\"\\nPackage: {type(x).__module__}.{type(x).__name__}\")\n",
    "    print(f\"dace is_array: {dace.dtypes.is_array(x)}\")\n",
    "    print(\"has __array_interface__:\", hasattr(x, \"__array_interface__\"))\n",
    "    print(\"has __cuda_array_interface__:\", hasattr(x, \"__cuda_array_interface__\"))\n",
    "    print(\"has __dlpack__:\", hasattr(x, \"__dlpack__\") or hasattr(x, \"__torch_dlpack__\"))\n",
    "def show_desc(obj):\n",
    "    print(f\"\\nPackage: {type(obj).__module__}.{type(obj).__name__}, is_array={dace.dtypes.is_array(obj)}\")\n",
    "    try:\n",
    "        desc = dace.data.create_datadescriptor(obj)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create descriptor for {type(obj).__name__}: {e}\")\n",
    "        return\n",
    "    \n",
    "    # print(f\"Object: {type(obj).__name__}, is_array={dace.dtypes.is_array(obj)}\")\n",
    "    print(f\"  Descriptor type: {type(desc).__name__}, dtype={desc.dtype}, shape={getattr(desc,'shape',None)}\")\n",
    "\n",
    "\n",
    "def has_cpu_array_iface(x):  # zero-copy on CPU?\n",
    "    return hasattr(x, \"__array_interface__\")\n",
    "\n",
    "def has_cuda_array_iface(x):  # zero-copy on GPU?\n",
    "    return hasattr(x, \"__cuda_array_interface__\")\n",
    "def dtype_info(obj):\n",
    " \n",
    "    print(f\"\\n{obj.to_string()}\")\n",
    "    print(f\" - type: {obj.type}\")\n",
    "    print(f\" - dtype: {obj.dtype}\")\n",
    "    print(f\" - ctype: {obj.ctype}\")\n",
    "    print(f\" - ctype unaligned: {obj.ctype_unaligned}\")\n",
    "    print(f\" - ocl type: {obj.ocltype}\")\n",
    "    # print(f\" - numpy: {obj.as_numpy_dtype()}\")\n",
    "    print(f\" - veclen: {obj.veclen}\")\n",
    "    print(f\" - bytes: {obj.bytes}\")\n",
    "    # print(f\".- as_arg: {obj.as_arg()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cebc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeclassAttributeError(AttributeError):\n",
    "    def __init__(self, obj, attr, original_exc):\n",
    "        public_attrs = [a for a in dir(obj) if not a.startswith('_')]\n",
    "        msg = (\n",
    "            f\"Failed to access attribute '{attr}' on object {obj!r}: {original_exc}\\n\"\n",
    "            f\"Available public attributes ({len(public_attrs)}): {', '.join(public_attrs)}\"\n",
    "        )\n",
    "        super().__init__(msg)\n",
    "        self.obj = obj\n",
    "        self.attr = attr\n",
    "        self.available = public_attrs\n",
    "        self.original_exc = original_exc\n",
    "\n",
    "\n",
    "\n",
    "def collect_typeclass_rows(typeclasses):\n",
    "    \"\"\"\n",
    "    Return list of dicts describing each DaCe typeclass.\n",
    "    If accessing any requested attribute fails, raise TypeclassAttributeError\n",
    "    that lists the object's available public attributes.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for dt in typeclasses:\n",
    "        # Access each field via guarded helpers (so any failure produces rich debug info)\n",
    "        def _safe_get(attr, default=None):\n",
    "            \n",
    "            try:\n",
    "                return getattr(dt, attr)\n",
    "            except Exception as e:\n",
    "                if default is not None:\n",
    "                    return default\n",
    "                raise TypeclassAttributeError(dt, attr, e) from e\n",
    "        row = {'obj': dt}\n",
    "        # Simple attributes\n",
    "        \n",
    "        for name in [\"type\", \"dtype\",\"base_type\", \"veclen\", \"bytes\",\n",
    "                     \"ctype\", \"ctype_unaligned\"]:\n",
    "            row[name] = _safe_get(name)\n",
    "        row['ocltype'] = _safe_get('ocltype', '-')\n",
    "        row['vtype'] = _safe_get('vtype', '-')\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            row['ctypes'] = dt.as_ctypes()\n",
    "        except Exception:\n",
    "            row['ctypes'] = '-'\n",
    "            \n",
    "        for name in [ \"ocltype\"]:\n",
    "            try:\n",
    "                row[name] = _safe_get(dt, name)\n",
    "            except TypeclassAttributeError:\n",
    "                row[name] = \"-\"\n",
    "        # Derived / method-based\n",
    "        try:\n",
    "            row[\"numpy\"] = dt.as_numpy_dtype()\n",
    "        except Exception as e:\n",
    "            row[\"numpy\"] = '-'\n",
    "            # print(dt.as_numpy_dtype())\n",
    "            # raise TypeclassAttributeError(dt, \"as_numpy_dtype()\", e) from e\n",
    "        \n",
    "        try:\n",
    "            row[\"json\"] = dt.to_json()\n",
    "        except Exception as e:\n",
    "            raise TypeclassAttributeError(dt, \"to_json()\", e) from e\n",
    "        # try:\n",
    "        #     row[\"to_string\"] = dt.to_string()\n",
    "        # except Exception as e:\n",
    "        #     raise TypeclassAttributeError(dt, \"to_string()\", e) from e\n",
    "        \n",
    "        # try:\n",
    "        #     row[\"data_desc\"] = dt.create_datadescriptor()\n",
    "        # except Exception as e:\n",
    "        #     raise TypeclassAttributeError(dt, \"as_arg()\", e) from e\n",
    "        try:\n",
    "            row[\"vtype\"] = dt.vtype\n",
    "        except Exception as e:\n",
    "            row['vtype'] = '-'\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def print_typeclass_rows(rows, *columns):\n",
    "    \"\"\"\n",
    "    Print a table of selected columns (in given order).\n",
    "    Args:\n",
    "        rows: list[dict] from collect_typeclass_rows\n",
    "        *columns: column names; if empty, all columns in row order.\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        print(\"(no rows)\")\n",
    "        return\n",
    "    if not columns:\n",
    "        columns = tuple(rows[0].keys())\n",
    "    missing = [c for c in columns if c not in rows[0]]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Unknown column(s): {missing}\")\n",
    "    widths = {c: max(len(c), *(len(str(r[c])) for r in rows)) for c in columns}\n",
    "    fmt = \"  \".join(f\"{{:{widths[c]}s}}\" for c in columns)\n",
    "    print(fmt.format(*columns))\n",
    "    print(\"  \".join(\"-\" * widths[c] for c in columns))\n",
    "    for r in rows:\n",
    "        print(fmt.format(*(str(r[c]) for c in columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5eb38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_typeclass_info_row(tc, print_info=False, columns=[\"obj\",\"base_type\", \"json\",\"ctypes\", \"ctype\", \"numpy\",\"bytes\"]):\n",
    "    \"\"\"\n",
    "    Return a dict describing a single DaCe typeclass `tc`.\n",
    "\n",
    "    Args:\n",
    "        tc: a DaCe typeclass instance\n",
    "        print_info (bool): if True, print the row nicely\n",
    "        columns (list[str] | None): list of column names to print (when print_info=True).\n",
    "                                    If None, print all available columns in row order.\n",
    "\n",
    "    Returns:\n",
    "        dict: mapping column name -> value\n",
    "    \"\"\"\n",
    "    def _safe(attr, default='-'):\n",
    "        try:\n",
    "            return getattr(tc, attr)\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "    row = {'obj': tc}\n",
    "    for name in [\"type\", \"dtype\", \"base_type\", \"veclen\", \"bytes\", \"ctype\", \"ctype_unaligned\"]:\n",
    "        row[name] = _safe(name)\n",
    "\n",
    "    # optional / method-based attributes\n",
    "    row['ocltype'] = _safe('ocltype', '-')\n",
    "    row['vtype'] = _safe('vtype', '-')\n",
    "    \n",
    "    try:\n",
    "        row['ctypes'] = tc.as_ctypes()\n",
    "    except Exception:\n",
    "        row['ctypes'] = '-'\n",
    "    try:\n",
    "        row['numpy'] = tc.as_numpy_dtype()\n",
    "    except Exception:\n",
    "        row['numpy'] = '-'\n",
    "    \n",
    "    try:\n",
    "        row['json'] = tc.to_json()\n",
    "    except Exception:\n",
    "        row['json'] = '-'\n",
    "  \n",
    "    if print_info:\n",
    "        if columns is None:\n",
    "            columns_to_print = list(row.keys())\n",
    "        else:\n",
    "            # validate requested columns\n",
    "            missing = [c for c in columns if c not in row]\n",
    "            if missing:\n",
    "                raise ValueError(f\"Unknown column(s): {missing}\")\n",
    "            columns_to_print = list(columns)\n",
    "\n",
    "        widths = {c: max(len(str(c)), len(str(row[c]))) for c in columns_to_print}\n",
    "        fmt = \"  \".join(f\"{{:{widths[c]}s}}\" for c in columns_to_print)\n",
    "        print(fmt.format(*columns_to_print))\n",
    "        print(\"  \".join(\"-\" * widths[c] for c in columns_to_print))\n",
    "        print(fmt.format(*(str(row[c]) for c in columns_to_print)))\n",
    "\n",
    "    return row\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef194ea",
   "metadata": {},
   "source": [
    "# Part 1: Primitive types and platform enums (`dace.dtypes`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f82ef5",
   "metadata": {},
   "source": [
    "### Practical tips & next steps\n",
    "\n",
    "- Use `dace.dtypes` when declaring element types in annotations or when you need a type-level object you can serialize or inspect. Use `dace.data` descriptors when you need to describe memory layout or add arrays to an SDFG.\n",
    "\n",
    "- To integrate new external array-like types with DaCe, implement `__descriptor__()` on the wrapper type and return a `dace.data.Array`/`Scalar` as appropriate. This is the most robust integration point.\n",
    "\n",
    "- For runtime testing, `make_array_from_descriptor()` creates a NumPy/CuPy buffer matching a descriptor—useful for unit tests that validate codegen IO shapes.\n",
    "\n",
    "Next steps you might want me to do:\n",
    "- Add a short example that declares a `@dace.program` function that accepts `dace.float32[M,N]` and demonstrate how the runtime buffer maps to the descriptor.\n",
    "- Add an example showing `dace.vector(...)` in an SDFG or how `dtype_to_typeclass()` maps external dtype objects into DaCe typeclasses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b379c41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default accepted dtype: [<class 'bool'>, <class 'int'>, <class 'float'>, <class 'complex'>, <class 'numpy.bool_'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>, <class 'numpy.uint8'>, <class 'numpy.uint16'>, <class 'numpy.uint32'>, <class 'numpy.uint64'>, <class 'numpy.float16'>, <class 'numpy.float32'>, <class 'numpy.float64'>, <class 'numpy.complex64'>, <class 'numpy.complex128'>, <class 'numpy.longlong'>, <class 'numpy.ulonglong'>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[bool,\n",
       " int,\n",
       " float,\n",
       " complex,\n",
       " numpy.bool_,\n",
       " numpy.int8,\n",
       " numpy.int16,\n",
       " numpy.int32,\n",
       " numpy.int64,\n",
       " numpy.uint8,\n",
       " numpy.uint16,\n",
       " numpy.uint32,\n",
       " numpy.uint64,\n",
       " numpy.float16,\n",
       " numpy.float32,\n",
       " numpy.float64,\n",
       " numpy.complex64,\n",
       " numpy.complex128,\n",
       " numpy.longlong,\n",
       " numpy.ulonglong]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_typeclass_objs = dtypes.dtype_to_typeclass()\n",
    "dtype_keys = list(registered_typeclass_objs.keys())\n",
    "registered_dace_dtypes = list(registered_typeclass_objs.values())\n",
    "# print(registered_dace_dtypes)\n",
    "print(f\"default accepted dtype: {dtype_keys}\")\n",
    "dtype_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8715c03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builtins.bool ->  bool (src=dace.dtypes.bool)\n",
      "<class 'bool'>\n",
      "builtins.int ->  int64_t (src=dace.dtypes.int64_t)\n",
      "<class 'int'>\n",
      "builtins.float ->  double (src=dace.dtypes.double)\n",
      "<class 'float'>\n",
      "builtins.complex ->  dace::complex128 (src=dace.dtypes.dace::complex128)\n",
      "<class 'complex'>\n",
      "numpy.bool_ ->  bool (src=dace.dtypes.bool)\n",
      "<class 'numpy.bool_'>\n",
      "numpy.int8 ->  char (src=dace.dtypes.char)\n",
      "<class 'numpy.int8'>\n",
      "numpy.int16 ->  short (src=dace.dtypes.short)\n",
      "<class 'numpy.int16'>\n",
      "numpy.int32 ->  int (src=dace.dtypes.int)\n",
      "<class 'numpy.int32'>\n",
      "numpy.int64 ->  int64_t (src=dace.dtypes.int64_t)\n",
      "<class 'numpy.int64'>\n",
      "numpy.uint8 ->  uint8_t (src=dace.dtypes.uint8_t)\n",
      "<class 'numpy.uint8'>\n",
      "numpy.uint16 ->  uint16_t (src=dace.dtypes.uint16_t)\n",
      "<class 'numpy.uint16'>\n",
      "numpy.uint32 ->  dace::uint (src=dace.dtypes.dace::uint)\n",
      "<class 'numpy.uint32'>\n",
      "numpy.uint64 ->  uint64_t (src=dace.dtypes.uint64_t)\n",
      "<class 'numpy.uint64'>\n",
      "numpy.float16 ->  dace::float16 (src=dace.dtypes.dace::float16)\n",
      "<class 'numpy.float16'>\n",
      "numpy.float32 ->  float (src=dace.dtypes.float)\n",
      "<class 'numpy.float32'>\n",
      "numpy.float64 ->  double (src=dace.dtypes.double)\n",
      "<class 'numpy.float64'>\n",
      "numpy.complex64 ->  dace::complex64 (src=dace.dtypes.dace::complex64)\n",
      "<class 'numpy.complex64'>\n",
      "numpy.complex128 ->  dace::complex128 (src=dace.dtypes.dace::complex128)\n",
      "<class 'numpy.complex128'>\n",
      "numpy.longlong ->  int64_t (src=dace.dtypes.int64_t)\n",
      "<class 'numpy.longlong'>\n",
      "numpy.ulonglong ->  uint64_t (src=dace.dtypes.uint64_t)\n",
      "<class 'numpy.ulonglong'>\n",
      "obj               base_type         json        ocltype  ctype             ctypes                     numpy       bytes\n",
      "----------------  ----------------  ----------  -------  ----------------  -------------------------  ----------  -----\n",
      "bool              bool              bool        ocltype  bool              <class 'ctypes.c_bool'>    bool        1    \n",
      "int64_t           int64_t           int64       ocltype  int64_t           <class 'ctypes.c_long'>    int64       8    \n",
      "double            double            float64     ocltype  double            <class 'ctypes.c_double'>  float64     8    \n",
      "dace::complex128  dace::complex128  complex128  ocltype  dace::complex128  <class 'ctypes.c_double'>  complex128  16   \n",
      "bool              bool              bool        ocltype  bool              <class 'ctypes.c_bool'>    bool        1    \n",
      "char              char              int8        ocltype  char              <class 'ctypes.c_byte'>    int8        1    \n",
      "short             short             int16       ocltype  short             <class 'ctypes.c_short'>   int16       2    \n",
      "int               int               int32       ocltype  int               <class 'ctypes.c_int'>     int32       4    \n",
      "int64_t           int64_t           int64       ocltype  int64_t           <class 'ctypes.c_long'>    int64       8    \n",
      "uint8_t           uint8_t           uint8       ocltype  uint8_t           <class 'ctypes.c_ubyte'>   uint8       1    \n",
      "uint16_t          uint16_t          uint16      ocltype  uint16_t          <class 'ctypes.c_ushort'>  uint16      2    \n",
      "dace::uint        dace::uint        uint32      ocltype  dace::uint        <class 'ctypes.c_uint'>    uint32      4    \n",
      "uint64_t          uint64_t          uint64      ocltype  uint64_t          <class 'ctypes.c_ulong'>   uint64      8    \n",
      "dace::float16     dace::float16     float16     ocltype  dace::float16     <class 'ctypes.c_ushort'>  float16     2    \n",
      "float             float             float32     ocltype  float             <class 'ctypes.c_float'>   float32     4    \n",
      "double            double            float64     ocltype  double            <class 'ctypes.c_double'>  float64     8    \n",
      "dace::complex64   dace::complex64   complex64   ocltype  dace::complex64   <class 'ctypes.c_ulong'>   complex64   8    \n",
      "dace::complex128  dace::complex128  complex128  ocltype  dace::complex128  <class 'ctypes.c_double'>  complex128  16   \n",
      "int64_t           int64_t           int64       ocltype  int64_t           <class 'ctypes.c_long'>    int64       8    \n",
      "uint64_t          uint64_t          uint64      ocltype  uint64_t          <class 'ctypes.c_ulong'>   uint64      8    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key,val in registered_typeclass_objs.items():\n",
    "    print(f\"{key.__module__}.{key.__name__} ->  {val} (src={val.__module__}.{val})\")\n",
    "    print(f\"{key}\")\n",
    "\n",
    "    tc = dace.dtypes.dtype_to_typeclass(key)\n",
    "    # get_typeclass_info_row(tc, print_info=True)\n",
    "\n",
    "print_typeclass_rows(collect_typeclass_rows(registered_dace_dtypes), \"obj\",\"base_type\", \"json\",\"ocltype\", \"ctype\",\"ctypes\", \"numpy\",\"bytes\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45221b5d",
   "metadata": {},
   "source": [
    "Python types → DaCe types\n",
    "\n",
    "\n",
    "\n",
    "print_typeclass_rows(collect_typeclass_rows(registered_dace_dtypes), \"obj\",\"base_type\", \"json\",\"ocltype\", \"ctype\", \"numpy\",\"bytes\",)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "h(\"Primitive dtypes\")\n",
    "\n",
    "Python scalars and numpy scalars are automatically converted to DaCe Scalar descriptors \n",
    "\n",
    "\n",
    "print('\\nScalar canonicalization:')\n",
    "print('  create_datadescriptor(7) ->', type(data.create_datadescriptor(7)).__name__)\n",
    "print('  create_datadescriptor(np.int32(5)) ->', type(data.create_datadescriptor(np.int32(5))).__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0488e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bool'>: bool\n",
      "<class 'int'>: int64_t\n",
      "<class 'float'>: double\n",
      "<class 'complex'>: dace::complex128\n",
      "<class 'numpy.bool'>: bool\n",
      "<class 'numpy.int8'>: char\n",
      "<class 'numpy.int16'>: short\n",
      "<class 'numpy.int32'>: int\n",
      "<class 'numpy.int64'>: int64_t\n",
      "<class 'numpy.uint8'>: uint8_t\n",
      "<class 'numpy.uint16'>: uint16_t\n",
      "<class 'numpy.uint32'>: dace::uint\n",
      "<class 'numpy.uint64'>: uint64_t\n",
      "<class 'numpy.float16'>: dace::float16\n",
      "<class 'numpy.float32'>: float\n",
      "<class 'numpy.float64'>: double\n",
      "<class 'numpy.complex64'>: dace::complex64\n",
      "<class 'numpy.complex128'>: dace::complex128\n",
      "<class 'numpy.longlong'>: int64_t\n",
      "<class 'numpy.ulonglong'>: uint64_t\n"
     ]
    }
   ],
   "source": [
    "for k,v in registered_typeclass_objs.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa4f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================\n",
      " Built-in types\n",
      "========================================================================\n"
     ]
    },
    {
     "ename": "TypeclassAttributeError",
     "evalue": "Failed to access attribute 'type' on object <class 'int'>: type object 'int' has no attribute 'type'\nAvailable public attributes (11): as_integer_ratio, bit_count, bit_length, conjugate, denominator, from_bytes, imag, is_integer, numerator, real, to_bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36m_safe_get\u001b[39m\u001b[34m(obj, attr)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'int' has no attribute 'type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeclassAttributeError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# builtin_\u001b[39;00m\n\u001b[32m      6\u001b[39m h(\u001b[33m\"\u001b[39m\u001b[33mBuilt-in types\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m _pytypes = \u001b[43mcollect_typeclass_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuiltin_python_scalar_dtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m print_typeclass_rows(_pytypes, \u001b[33m\"\u001b[39m\u001b[33mobj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_string\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mocltype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mctype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mctypes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mcollect_typeclass_rows\u001b[39m\u001b[34m(typeclasses)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Simple attributes\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mbase_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mveclen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbytes\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m              \u001b[33m\"\u001b[39m\u001b[33mctype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mctype_unaligned\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     row[name] = \u001b[43m_safe_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [ \u001b[33m\"\u001b[39m\u001b[33mocltype\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36m_safe_get\u001b[39m\u001b[34m(obj, attr)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, attr)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TypeclassAttributeError(obj, attr, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mTypeclassAttributeError\u001b[39m: Failed to access attribute 'type' on object <class 'int'>: type object 'int' has no attribute 'type'\nAvailable public attributes (11): as_integer_ratio, bit_count, bit_length, conjugate, denominator, from_bytes, imag, is_integer, numerator, real, to_bytes"
     ]
    }
   ],
   "source": [
    "def debug_builtin_dtype_info(dt):\n",
    "    print(f\"\\n {dt.__module__}.{dt}: {dt} ({type(dt).__name__})\")\n",
    "    try:\n",
    "        row = get_typeclass_info_row(dt, print_info=True)\n",
    "    except TypeclassAttributeError as e:\n",
    "        print(f\"Error accessing typeclass attributes: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return\n",
    "    return row\n",
    "builtin_python_scalar_dtypes = [int, float, complex, bool,str, bytes]\n",
    "# builtin_\n",
    "\n",
    "\n",
    "\n",
    "h(\"Built-in types\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ba6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_pytypes = collect_typeclass_rows(builtin_python_scalar_dtypes)\n",
    "\n",
    "print_typeclass_rows(_pytypes, \"obj\", \"label\", \"to_string\", \"ocltype\", \"ctype\", \"ctypes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08325799",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03015d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default register dtype keys:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[bool,\n",
       " int,\n",
       " float,\n",
       " complex,\n",
       " numpy.bool,\n",
       " numpy.int8,\n",
       " numpy.int16,\n",
       " numpy.int32,\n",
       " numpy.int64,\n",
       " numpy.uint8,\n",
       " numpy.uint16,\n",
       " numpy.uint32,\n",
       " numpy.uint64,\n",
       " numpy.float16,\n",
       " numpy.float32,\n",
       " numpy.float64,\n",
       " numpy.complex64,\n",
       " numpy.complex128,\n",
       " numpy.longlong,\n",
       " numpy.ulonglong]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c153a9a0",
   "metadata": {},
   "source": [
    "Python types → DaCe types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2226e84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builtins.bool ->  bool (dace.dtypes.bool)\n",
      "builtins.int ->  int64_t (dace.dtypes.int64_t)\n",
      "builtins.float ->  double (dace.dtypes.double)\n",
      "builtins.complex ->  dace::complex128 (dace.dtypes.dace::complex128)\n",
      "numpy.bool ->  bool (dace.dtypes.bool)\n",
      "numpy.int8 ->  char (dace.dtypes.char)\n",
      "numpy.int16 ->  short (dace.dtypes.short)\n",
      "numpy.int32 ->  int (dace.dtypes.int)\n",
      "numpy.int64 ->  int64_t (dace.dtypes.int64_t)\n",
      "numpy.uint8 ->  uint8_t (dace.dtypes.uint8_t)\n",
      "numpy.uint16 ->  uint16_t (dace.dtypes.uint16_t)\n",
      "numpy.uint32 ->  dace::uint (dace.dtypes.dace::uint)\n",
      "numpy.uint64 ->  uint64_t (dace.dtypes.uint64_t)\n",
      "numpy.float16 ->  dace::float16 (dace.dtypes.dace::float16)\n",
      "numpy.float32 ->  float (dace.dtypes.float)\n",
      "numpy.float64 ->  double (dace.dtypes.double)\n",
      "numpy.complex64 ->  dace::complex64 (dace.dtypes.dace::complex64)\n",
      "numpy.complex128 ->  dace::complex128 (dace.dtypes.dace::complex128)\n",
      "numpy.longlong ->  int64_t (dace.dtypes.int64_t)\n",
      "numpy.ulonglong ->  uint64_t (dace.dtypes.uint64_t)\n",
      "obj               base_type         json        ocltype         ctype             numpy       bytes\n",
      "----------------  ----------------  ----------  --------------  ----------------  ----------  -----\n",
      "bool              bool              bool        bool            bool              bool        1    \n",
      "int64_t           int64_t           int64       long            int64_t           int64       8    \n",
      "double            double            float64     double          double            float64     8    \n",
      "dace::complex128  dace::complex128  complex128  complex double  dace::complex128  complex128  16   \n",
      "bool              bool              bool        bool            bool              bool        1    \n",
      "char              char              int8        char            char              int8        1    \n",
      "short             short             int16       short           short             int16       2    \n",
      "int               int               int32       int             int               int32       4    \n",
      "int64_t           int64_t           int64       long            int64_t           int64       8    \n",
      "uint8_t           uint8_t           uint8       uchar           uint8_t           uint8       1    \n",
      "uint16_t          uint16_t          uint16      ushort          uint16_t          uint16      2    \n",
      "dace::uint        dace::uint        uint32      uint            dace::uint        uint32      4    \n",
      "uint64_t          uint64_t          uint64      ulong           uint64_t          uint64      8    \n",
      "dace::float16     dace::float16     float16     -               dace::float16     float16     2    \n",
      "float             float             float32     float           float             float32     4    \n",
      "double            double            float64     double          double            float64     8    \n",
      "dace::complex64   dace::complex64   complex64   complex float   dace::complex64   complex64   8    \n",
      "dace::complex128  dace::complex128  complex128  complex double  dace::complex128  complex128  16   \n",
      "int64_t           int64_t           int64       long            int64_t           int64       8    \n",
      "uint64_t          uint64_t          uint64      ulong           uint64_t          uint64      8    \n"
     ]
    }
   ],
   "source": [
    "for key,val in registered_typeclass_objs.items():\n",
    "    print(f\"{key.__module__}.{key.__name__} ->  {val} ({val.__module__}.{val})\")\n",
    "\n",
    "    tc = dace.dtypes.dtype_to_typeclass(key)\n",
    "    # get_typeclass_info_row(tc, print_info=True)\n",
    "\n",
    "print_typeclass_rows(collect_typeclass_rows(registered_dace_dtypes), \"obj\",\"base_type\", \"json\",\"ocltype\", \"ctype\", \"numpy\",\"bytes\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d26b088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj               base_type         json        ocltype         ctype             numpy       bytes\n",
      "----------------  ----------------  ----------  --------------  ----------------  ----------  -----\n",
      "bool              bool              bool        bool            bool              bool        1    \n",
      "int64_t           int64_t           int64       long            int64_t           int64       8    \n",
      "double            double            float64     double          double            float64     8    \n",
      "dace::complex128  dace::complex128  complex128  complex double  dace::complex128  complex128  16   \n",
      "bool              bool              bool        bool            bool              bool        1    \n",
      "char              char              int8        char            char              int8        1    \n",
      "short             short             int16       short           short             int16       2    \n",
      "int               int               int32       int             int               int32       4    \n",
      "int64_t           int64_t           int64       long            int64_t           int64       8    \n",
      "uint8_t           uint8_t           uint8       uchar           uint8_t           uint8       1    \n",
      "uint16_t          uint16_t          uint16      ushort          uint16_t          uint16      2    \n",
      "dace::uint        dace::uint        uint32      uint            dace::uint        uint32      4    \n",
      "uint64_t          uint64_t          uint64      ulong           uint64_t          uint64      8    \n",
      "dace::float16     dace::float16     float16     -               dace::float16     float16     2    \n",
      "float             float             float32     float           float             float32     4    \n",
      "double            double            float64     double          double            float64     8    \n",
      "dace::complex64   dace::complex64   complex64   complex float   dace::complex64   complex64   8    \n",
      "dace::complex128  dace::complex128  complex128  complex double  dace::complex128  complex128  16   \n",
      "int64_t           int64_t           int64       long            int64_t           int64       8    \n",
      "uint64_t          uint64_t          uint64      ulong           uint64_t          uint64      8    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_typeclass_rows(collect_typeclass_rows(registered_dace_dtypes), \"obj\",\"base_type\", \"json\",\"ocltype\", \"ctype\", \"numpy\",\"bytes\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d96336fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================\n",
      " Primitive dtypes\n",
      "========================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown column(s): ['ctypes']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     14\u001b[39m _all_rows = collect_typeclass_rows(primitive_types)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Example usages:\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# print(\"Full set (default):\")\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# print_typeclass_rows(_all_rows)  # all columns\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# print(\"\\nSelected columns (label, bytes, veclen, numpy, ctype):\")\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print_typeclass_rows(_all_rows, \"label\", \"bytes\", \"veclen\", \"numpy\", \"ctype\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mprint_typeclass_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_all_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mobj\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbase_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mocltype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mctype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mctypes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbytes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mprint_typeclass_rows\u001b[39m\u001b[34m(rows, *columns)\u001b[39m\n\u001b[32m     80\u001b[39m missing = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m rows[\u001b[32m0\u001b[39m]]\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown column(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m widths = {c: \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(c), *(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(r[c])) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rows)) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m columns}\n\u001b[32m     84\u001b[39m fmt = \u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidths[c]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m columns)\n",
      "\u001b[31mValueError\u001b[39m: Unknown column(s): ['ctypes']"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "# Primitive typeclasses table with selectable columns helper\n",
    "\n",
    "primitive_types = [\n",
    "    dtypes.int8,dtypes.int16,dtypes.int32,dtypes.int64, \n",
    "    dtypes.uintp, dtypes.uint32,\n",
    "    dtypes.float16,dtypes.float32, dtypes.float64,\n",
    "    dtypes.complex64, dtypes.complex128, dtypes.string\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "h(\"Primitive dtypes\")\n",
    "\n",
    "_all_rows = collect_typeclass_rows(primitive_types)\n",
    "\n",
    "# Example usages:\n",
    "# print(\"Full set (default):\")\n",
    "# print_typeclass_rows(_all_rows)  # all columns\n",
    "\n",
    "# print(\"\\nSelected columns (label, bytes, veclen, numpy, ctype):\")\n",
    "# print_typeclass_rows(_all_rows, \"label\", \"bytes\", \"veclen\", \"numpy\", \"ctype\")\n",
    "\n",
    "print_typeclass_rows(_all_rows, \"obj\",\"base_type\", \"json\",\"ocltype\", \"ctype\", \"ctypes\",\"bytes\",)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h(\"Primitive dtypes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeed39b",
   "metadata": {},
   "source": [
    "Python scalars and numpy scalars are automatically converted to DaCe Scalar descriptors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66c5157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scalar canonicalization:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mScalar canonicalization:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m  create_datadescriptor(7) ->\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m(\u001b[43mdata\u001b[49m.create_datadescriptor(\u001b[32m7\u001b[39m)).\u001b[34m__name__\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m  create_datadescriptor(np.int32(5)) ->\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m(data.create_datadescriptor(np.int32(\u001b[32m5\u001b[39m))).\u001b[34m__name__\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\nScalar canonicalization:')\n",
    "print('  create_datadescriptor(7) ->', type(data.create_datadescriptor(7)).__name__)\n",
    "print('  create_datadescriptor(np.int32(5)) ->', type(data.create_datadescriptor(np.int32(5))).__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e4928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================\n",
      " Built-in types\n",
      "========================================================================\n"
     ]
    },
    {
     "ename": "TypeclassAttributeError",
     "evalue": "Failed to access attribute 'type' on object <class 'int'>: type object 'int' has no attribute 'type'\nAvailable public attributes (11): as_integer_ratio, bit_count, bit_length, conjugate, denominator, from_bytes, imag, is_integer, numerator, real, to_bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36m_safe_get\u001b[39m\u001b[34m(obj, attr)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'int' has no attribute 'type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeclassAttributeError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m builtin_dtypes = [\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m, \u001b[38;5;28mbool\u001b[39m,\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m,\u001b[38;5;28mlist\u001b[39m,\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mset\u001b[39m, \u001b[38;5;28mdict\u001b[39m]\n\u001b[32m      5\u001b[39m h(\u001b[33m\"\u001b[39m\u001b[33mBuilt-in types\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m _pytypes = \u001b[43mcollect_typeclass_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuiltin_dtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m print_typeclass_rows(_pytypes, \u001b[33m\"\u001b[39m\u001b[33mobj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_string\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mocltype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mctype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbytes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mcollect_typeclass_rows\u001b[39m\u001b[34m(typeclasses)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Simple attributes\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mbase_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mveclen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbytes\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m              \u001b[33m\"\u001b[39m\u001b[33mctype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mctype_unaligned\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     row[name] = \u001b[43m_safe_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [ \u001b[33m\"\u001b[39m\u001b[33mocltype\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36m_safe_get\u001b[39m\u001b[34m(obj, attr)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, attr)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TypeclassAttributeError(obj, attr, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mTypeclassAttributeError\u001b[39m: Failed to access attribute 'type' on object <class 'int'>: type object 'int' has no attribute 'type'\nAvailable public attributes (11): as_integer_ratio, bit_count, bit_length, conjugate, denominator, from_bytes, imag, is_integer, numerator, real, to_bytes"
     ]
    }
   ],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ece9d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('a', '<i4'), ('b', '<f8')], align=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struc =  dtypes.struct('Pair', a=dtypes.int32, b=dtypes.float64)\n",
    "struc.as_numpy_dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9869dc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================\n",
      " vector typeclasses\n",
      "========================================================================\n",
      "obj                   type                     json                                                     ocltype  veclen  bytes\n",
      "--------------------  -----------------------  -------------------------------------------------------  -------  ------  -----\n",
      "dace::vec<int, 4>     <class 'numpy.int32'>    {'type': 'vector', 'dtype': 'int32', 'elements': '4'}    int4     4       16   \n",
      "dace::vec<double, 3>  <class 'numpy.float64'>  {'type': 'vector', 'dtype': 'float64', 'elements': '3'}  double3  3       24   \n"
     ]
    }
   ],
   "source": [
    "vector_types = [\n",
    "    dace.vector(dace.int32, 4),\n",
    "    dace.vector(dace.float64, 3),\n",
    "    \n",
    "   ]\n",
    "struc_types = [dtypes.struct('Pair', a=dtypes.int32, b=dtypes.float64),\n",
    "               ]\n",
    "pointer_types = [dtypes.pointer(dace.int32),\n",
    "                 dtypes.pointer(dace.float64),\n",
    "                ]\n",
    "h(\"vector typeclasses\")\n",
    "vector_rows = collect_typeclass_rows(vector_types)\n",
    "print_typeclass_rows(vector_rows, \"obj\", \"type\", \"json\", \"ocltype\", \"veclen\",\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "44f0cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<f8', (3,))\n",
      "double3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'vector', 'dtype': 'float64', 'elements': '3'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = dace.vector(dace.int32, 4)\n",
    "vec = dace.vector(dace.float64, 3)\n",
    "print(vec.as_numpy_dtype())\n",
    "print(vec.ocltype)\n",
    "vec.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d42a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51142c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([1])\n",
    "assert x.byte().dtype ==  torch.uint8\n",
    "\n",
    "assert x.bool().dtype == torch.bool\n",
    "assert x.char().dtype == torch.int8\n",
    "assert x.double().dtype == torch.float64\n",
    "assert x.float().dtype == torch.float32\n",
    "assert x.half().dtype == torch.float16\n",
    "assert x.int().dtype == torch.int32\n",
    "assert x.bfloat16().dtype == torch.bfloat16\n",
    "\n",
    "cfloat = x.cfloat()\n",
    "assert cfloat.dtype == torch.complex64\n",
    "assert cfloat.real == x.float()\n",
    "assert cfloat.imag ==  torch.zeros_like(cfloat.imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "15b83ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__annotations__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__complex__',\n",
       " '__contains__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__dlpack__',\n",
       " '__dlpack_device__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__firstlineno__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ifloordiv__',\n",
       " '__ilshift__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rlshift__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rrshift__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__static_attributes__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__torch_dispatch__',\n",
       " '__torch_function__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_addmm_activation',\n",
       " '_autocast_to_full_precision',\n",
       " '_autocast_to_reduced_precision',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_clear_non_serializable_cached_data',\n",
       " '_coalesced_',\n",
       " '_conj',\n",
       " '_conj_physical',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_fix_weakref',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_has_symbolic_sizes_strides',\n",
       " '_indices',\n",
       " '_is_all_true',\n",
       " '_is_any_true',\n",
       " '_is_view',\n",
       " '_is_zerotensor',\n",
       " '_lazy_clone',\n",
       " '_make_subclass',\n",
       " '_make_wrapper_subclass',\n",
       " '_neg_view',\n",
       " '_nested_tensor_size',\n",
       " '_nested_tensor_storage_offsets',\n",
       " '_nested_tensor_strides',\n",
       " '_nnz',\n",
       " '_post_accumulate_grad_hooks',\n",
       " '_python_dispatch',\n",
       " '_reduce_ex_internal',\n",
       " '_rev_view_func_unsafe',\n",
       " '_sparse_mask_projection',\n",
       " '_to_dense',\n",
       " '_to_sparse',\n",
       " '_to_sparse_bsc',\n",
       " '_to_sparse_bsr',\n",
       " '_to_sparse_csc',\n",
       " '_to_sparse_csr',\n",
       " '_typed_storage',\n",
       " '_update_names',\n",
       " '_use_count',\n",
       " '_values',\n",
       " '_version',\n",
       " '_view_func',\n",
       " '_view_func_unsafe',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'absolute',\n",
       " 'absolute_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'acosh',\n",
       " 'acosh_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'adjoint',\n",
       " 'align_as',\n",
       " 'align_to',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'aminmax',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'arccos',\n",
       " 'arccos_',\n",
       " 'arccosh',\n",
       " 'arccosh_',\n",
       " 'arcsin',\n",
       " 'arcsin_',\n",
       " 'arcsinh',\n",
       " 'arcsinh_',\n",
       " 'arctan',\n",
       " 'arctan2',\n",
       " 'arctan2_',\n",
       " 'arctan_',\n",
       " 'arctanh',\n",
       " 'arctanh_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'argwhere',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'as_strided_scatter',\n",
       " 'as_subclass',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'asinh',\n",
       " 'asinh_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'atanh',\n",
       " 'atanh_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_and_',\n",
       " 'bitwise_left_shift',\n",
       " 'bitwise_left_shift_',\n",
       " 'bitwise_not',\n",
       " 'bitwise_not_',\n",
       " 'bitwise_or',\n",
       " 'bitwise_or_',\n",
       " 'bitwise_right_shift',\n",
       " 'bitwise_right_shift_',\n",
       " 'bitwise_xor',\n",
       " 'bitwise_xor_',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'broadcast_to',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'ccol_indices',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'cfloat',\n",
       " 'chalf',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clip',\n",
       " 'clip_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'col_indices',\n",
       " 'conj',\n",
       " 'conj_physical',\n",
       " 'conj_physical_',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'copysign',\n",
       " 'copysign_',\n",
       " 'corrcoef',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'count_nonzero',\n",
       " 'cov',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'crow_indices',\n",
       " 'cuda',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumprod_',\n",
       " 'cumsum',\n",
       " 'cumsum_',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'deg2rad',\n",
       " 'deg2rad_',\n",
       " 'dense_dim',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diagonal_scatter',\n",
       " 'diff',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dim_order',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'divide',\n",
       " 'divide_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'exp2_',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fill_',\n",
       " 'fill_diagonal_',\n",
       " 'fix',\n",
       " 'fix_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float',\n",
       " 'float_power',\n",
       " 'float_power_',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'floor_divide',\n",
       " 'floor_divide_',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'frexp',\n",
       " 'gather',\n",
       " 'gcd',\n",
       " 'gcd_',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'greater',\n",
       " 'greater_',\n",
       " 'greater_equal',\n",
       " 'greater_equal_',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'has_names',\n",
       " 'hash_tensor',\n",
       " 'heaviside',\n",
       " 'heaviside_',\n",
       " 'histc',\n",
       " 'histogram',\n",
       " 'hsplit',\n",
       " 'hypot',\n",
       " 'hypot_',\n",
       " 'i0',\n",
       " 'i0_',\n",
       " 'igamma',\n",
       " 'igamma_',\n",
       " 'igammac',\n",
       " 'igammac_',\n",
       " 'imag',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_reduce',\n",
       " 'index_reduce_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'inner',\n",
       " 'int',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'ipu',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_conj',\n",
       " 'is_contiguous',\n",
       " 'is_cpu',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_inference',\n",
       " 'is_ipu',\n",
       " 'is_leaf',\n",
       " 'is_maia',\n",
       " 'is_meta',\n",
       " 'is_mkldnn',\n",
       " 'is_mps',\n",
       " 'is_mtia',\n",
       " 'is_neg',\n",
       " 'is_nested',\n",
       " 'is_nonzero',\n",
       " 'is_pinned',\n",
       " 'is_quantized',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'is_sparse_csr',\n",
       " 'is_vulkan',\n",
       " 'is_xla',\n",
       " 'is_xpu',\n",
       " 'isclose',\n",
       " 'isfinite',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'istft',\n",
       " 'item',\n",
       " 'itemsize',\n",
       " 'kron',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'lcm',\n",
       " 'lcm_',\n",
       " 'ldexp',\n",
       " 'ldexp_',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'less',\n",
       " 'less_',\n",
       " 'less_equal',\n",
       " 'less_equal_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logcumsumexp',\n",
       " 'logdet',\n",
       " 'logical_and',\n",
       " 'logical_and_',\n",
       " 'logical_not',\n",
       " 'logical_not_',\n",
       " 'logical_or',\n",
       " 'logical_or_',\n",
       " 'logical_xor',\n",
       " 'logical_xor_',\n",
       " 'logit',\n",
       " 'logit_',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstsq',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'mH',\n",
       " 'mT',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_exp',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'module_load',\n",
       " 'moveaxis',\n",
       " 'movedim',\n",
       " 'msort',\n",
       " 'mtia',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'multiply',\n",
       " 'multiply_',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'names',\n",
       " 'nan_to_num',\n",
       " 'nan_to_num_',\n",
       " 'nanmean',\n",
       " 'nanmedian',\n",
       " 'nanquantile',\n",
       " 'nansum',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'nbytes',\n",
       " 'ndim',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'negative',\n",
       " 'negative_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_empty_strided',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nextafter',\n",
       " 'nextafter_',\n",
       " 'nonzero',\n",
       " 'nonzero_static',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'not_equal',\n",
       " 'not_equal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'outer',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'positive',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'put',\n",
       " 'put_',\n",
       " 'q_per_channel_axis',\n",
       " 'q_per_channel_scales',\n",
       " 'q_per_channel_zero_points',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qr',\n",
       " 'qscheme',\n",
       " 'quantile',\n",
       " 'rad2deg',\n",
       " 'rad2deg_',\n",
       " 'random_',\n",
       " 'ravel',\n",
       " 'real',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'refine_names',\n",
       " 'register_hook',\n",
       " 'register_post_accumulate_grad_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'rename',\n",
       " 'rename_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'repeat_interleave',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'resize_as_sparse_',\n",
       " 'resolve_conj',\n",
       " 'resolve_neg',\n",
       " 'retain_grad',\n",
       " 'retains_grad',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'row_indices',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'scatter_reduce',\n",
       " 'scatter_reduce_',\n",
       " 'select',\n",
       " 'select_scatter',\n",
       " 'set_',\n",
       " 'sgn',\n",
       " 'sgn_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'signbit',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinc',\n",
       " 'sinc_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slice_inverse',\n",
       " 'slice_scatter',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'square',\n",
       " 'square_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'subtract',\n",
       " 'subtract_',\n",
       " 'sum',\n",
       " 'sum_to_size',\n",
       " 'svd',\n",
       " 'swapaxes',\n",
       " 'swapaxes_',\n",
       " 'swapdims',\n",
       " 'swapdims_',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'take_along_dim',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'tensor_split',\n",
       " 'tile',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_mkldnn',\n",
       " 'to_padded_tensor',\n",
       " 'to_sparse',\n",
       " 'to_sparse_bsc',\n",
       " 'to_sparse_bsr',\n",
       " 'to_sparse_coo',\n",
       " 'to_sparse_csc',\n",
       " 'to_sparse_csr',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'true_divide',\n",
       " 'true_divide_',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unflatten',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsafe_chunk',\n",
       " 'unsafe_split',\n",
       " 'unsafe_split_with_sizes',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'untyped_storage',\n",
       " 'values',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'vsplit',\n",
       " 'where',\n",
       " 'xlogy',\n",
       " 'xlogy_',\n",
       " 'xpu',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(cfloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "97de1d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cfloat: tensor([1.+0.j])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "where() received an invalid combination of arguments - got (), but expected one of:\n * (Tensor condition, Tensor other)\n * (Tensor condition, Number other)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[202]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m cfloat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfloat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m cfloat.where: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcfloat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtype(cfloat.imag): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(cfloat.imag)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: where() received an invalid combination of arguments - got (), but expected one of:\n * (Tensor condition, Tensor other)\n * (Tensor condition, Number other)\n"
     ]
    }
   ],
   "source": [
    "print(f\" cfloat: {cfloat}\")\n",
    "print(f\" cfloat.where: {cfloat.where()}\")\n",
    "print(f\"type(cfloat.imag): {type(cfloat.imag)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "53742073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cfloat.imag.va: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/w2tzbky134bg3g8mct87xb8c0000gn/T/ipykernel_29270/4253374895.py:1: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  print(f\" cfloat.imag.va: {cfloat.imag.var()}\")\n"
     ]
    }
   ],
   "source": [
    "print(f\" cfloat.imag.va: {cfloat.imag.var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b71a8d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cfloat.imag:  0\n",
      " 0\n",
      " 128\n",
      " 63\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.storage.UntypedStorage(device=cpu) of size 8]\n",
      "['__annotations__', '__class__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_byteswap', '_cdata', '_checkpoint_offset', '_expired', '_fake_device', '_fix_weakref', '_free_weak_ref', '_get_filename', '_get_shared_fd', '_new_shared', '_new_shared_cuda', '_new_shared_fd_cpu', '_new_shared_filename_cpu', '_new_using_fd_cpu', '_new_using_filename_cpu', '_new_with_file', '_new_with_weak_ptr', '_release_ipc_counter', '_release_ipc_counter_cuda', '_set_cdata', '_set_from_file', '_share_cuda_', '_share_fd_cpu_', '_share_filename_cpu_', '_shared_decref', '_shared_incref', '_to', '_weak_ref', '_write_file', 'bfloat16', 'bool', 'byte', 'byteswap', 'char', 'clone', 'complex_double', 'complex_float', 'copy_', 'cpu', 'cuda', 'data_ptr', 'device', 'double', 'element_size', 'filename', 'fill_', 'float', 'float8_e4m3fn', 'float8_e4m3fnuz', 'float8_e5m2', 'float8_e5m2fnuz', 'from_buffer', 'from_file', 'get_device', 'half', 'hpu', 'int', 'is_cuda', 'is_hpu', 'is_pinned', 'is_shared', 'is_sparse', 'is_sparse_csr', 'long', 'mps', 'nbytes', 'new', 'pin_memory', 'resizable', 'resize_', 'share_memory_', 'short', 'size', 'to', 'tolist', 'type', 'untyped']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\" cfloat.imag: {cfloat.imag.untyped_storage()}\")\n",
    "storage = cfloat.imag.untyped_storage()\n",
    "print(dir(storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "adee17d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device: mps, CPU device: cpu\n"
     ]
    }
   ],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "print(f\"MPS device: {mps_device}, CPU device: {cpu_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d6a027d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.backends' from '/Users/sophieblock/miniforge3/envs/dace/lib/python3.13/site-packages/torch/backends/__init__.py'>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b311672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_MPS: True, \n",
      "_MKL: False, \n",
      "_ACL: False, \n",
      "_XPU: False, \n",
      "_HPU: False, \n",
      "_CUDA: False\n"
     ]
    }
   ],
   "source": [
    "_MPS = torch.backends.mps.is_available()\n",
    "_MKL = torch.backends.mkl.is_available()\n",
    "_ACL = torch.backends.mkldnn.is_available() and torch.ops.mkldnn._is_mkldnn_acl_supported()\n",
    "_XPU = torch.xpu.is_available()\n",
    "\n",
    "_HPU = True if (hasattr(torch, \"hpu\") and torch.hpu.is_available()) else False\n",
    "_CUDA = torch.cuda.is_available()\n",
    "print(f\"_MPS: {_MPS}, \\n_MKL: {_MKL}, \\n_ACL: {_ACL}, \\n_XPU: {_XPU}, \\n_HPU: {_HPU}, \\n_CUDA: {_CUDA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1602d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = (1,)\n",
    "Mdim = (192,)\n",
    "Kdim = (196,)\n",
    "Bdim = (84,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e432ddae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[186]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m cuda0 = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m cuda2 = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda:2\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# GPU 2 (these are 0-indexed)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcuda0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# x.device is device(type='cuda', index=0)\u001b[39;00m\n\u001b[32m      9\u001b[39m y = torch.tensor([\u001b[32m1.\u001b[39m, \u001b[32m2.\u001b[39m]).cuda()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/dace/lib/python3.13/site-packages/torch/cuda/__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m     )\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "storage.cuda\n",
    "\n",
    "cuda = torch.device('cuda')     # Default CUDA device\n",
    "cuda0 = torch.device('cuda:0')\n",
    "cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n",
    "\n",
    "x = torch.tensor([1., 2.], device=cuda0)\n",
    "# x.device is device(type='cuda', index=0)\n",
    "y = torch.tensor([1., 2.]).cuda()\n",
    "# y.device is device(type='cuda', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "efd7abae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7293368b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__annotations__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__complex__',\n",
       " '__contains__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__dlpack__',\n",
       " '__dlpack_device__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__firstlineno__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ifloordiv__',\n",
       " '__ilshift__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rlshift__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rrshift__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__static_attributes__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__torch_dispatch__',\n",
       " '__torch_function__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_addmm_activation',\n",
       " '_autocast_to_full_precision',\n",
       " '_autocast_to_reduced_precision',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_clear_non_serializable_cached_data',\n",
       " '_coalesced_',\n",
       " '_conj',\n",
       " '_conj_physical',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_fix_weakref',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_has_symbolic_sizes_strides',\n",
       " '_indices',\n",
       " '_is_all_true',\n",
       " '_is_any_true',\n",
       " '_is_view',\n",
       " '_is_zerotensor',\n",
       " '_lazy_clone',\n",
       " '_make_subclass',\n",
       " '_make_wrapper_subclass',\n",
       " '_neg_view',\n",
       " '_nested_tensor_size',\n",
       " '_nested_tensor_storage_offsets',\n",
       " '_nested_tensor_strides',\n",
       " '_nnz',\n",
       " '_post_accumulate_grad_hooks',\n",
       " '_python_dispatch',\n",
       " '_reduce_ex_internal',\n",
       " '_rev_view_func_unsafe',\n",
       " '_sparse_mask_projection',\n",
       " '_to_dense',\n",
       " '_to_sparse',\n",
       " '_to_sparse_bsc',\n",
       " '_to_sparse_bsr',\n",
       " '_to_sparse_csc',\n",
       " '_to_sparse_csr',\n",
       " '_typed_storage',\n",
       " '_update_names',\n",
       " '_use_count',\n",
       " '_values',\n",
       " '_version',\n",
       " '_view_func',\n",
       " '_view_func_unsafe',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'absolute',\n",
       " 'absolute_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'acosh',\n",
       " 'acosh_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'adjoint',\n",
       " 'align_as',\n",
       " 'align_to',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'aminmax',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'arccos',\n",
       " 'arccos_',\n",
       " 'arccosh',\n",
       " 'arccosh_',\n",
       " 'arcsin',\n",
       " 'arcsin_',\n",
       " 'arcsinh',\n",
       " 'arcsinh_',\n",
       " 'arctan',\n",
       " 'arctan2',\n",
       " 'arctan2_',\n",
       " 'arctan_',\n",
       " 'arctanh',\n",
       " 'arctanh_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'argwhere',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'as_strided_scatter',\n",
       " 'as_subclass',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'asinh',\n",
       " 'asinh_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'atanh',\n",
       " 'atanh_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_and_',\n",
       " 'bitwise_left_shift',\n",
       " 'bitwise_left_shift_',\n",
       " 'bitwise_not',\n",
       " 'bitwise_not_',\n",
       " 'bitwise_or',\n",
       " 'bitwise_or_',\n",
       " 'bitwise_right_shift',\n",
       " 'bitwise_right_shift_',\n",
       " 'bitwise_xor',\n",
       " 'bitwise_xor_',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'broadcast_to',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'ccol_indices',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'cfloat',\n",
       " 'chalf',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clip',\n",
       " 'clip_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'col_indices',\n",
       " 'conj',\n",
       " 'conj_physical',\n",
       " 'conj_physical_',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'copysign',\n",
       " 'copysign_',\n",
       " 'corrcoef',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'count_nonzero',\n",
       " 'cov',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'crow_indices',\n",
       " 'cuda',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumprod_',\n",
       " 'cumsum',\n",
       " 'cumsum_',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'deg2rad',\n",
       " 'deg2rad_',\n",
       " 'dense_dim',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diagonal_scatter',\n",
       " 'diff',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dim_order',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'divide',\n",
       " 'divide_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'exp2_',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fill_',\n",
       " 'fill_diagonal_',\n",
       " 'fix',\n",
       " 'fix_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float',\n",
       " 'float_power',\n",
       " 'float_power_',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'floor_divide',\n",
       " 'floor_divide_',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'frexp',\n",
       " 'gather',\n",
       " 'gcd',\n",
       " 'gcd_',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'greater',\n",
       " 'greater_',\n",
       " 'greater_equal',\n",
       " 'greater_equal_',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'has_names',\n",
       " 'hash_tensor',\n",
       " 'heaviside',\n",
       " 'heaviside_',\n",
       " 'histc',\n",
       " 'histogram',\n",
       " 'hsplit',\n",
       " 'hypot',\n",
       " 'hypot_',\n",
       " 'i0',\n",
       " 'i0_',\n",
       " 'igamma',\n",
       " 'igamma_',\n",
       " 'igammac',\n",
       " 'igammac_',\n",
       " 'imag',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_reduce',\n",
       " 'index_reduce_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'inner',\n",
       " 'int',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'ipu',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_conj',\n",
       " 'is_contiguous',\n",
       " 'is_cpu',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_inference',\n",
       " 'is_ipu',\n",
       " 'is_leaf',\n",
       " 'is_maia',\n",
       " 'is_meta',\n",
       " 'is_mkldnn',\n",
       " 'is_mps',\n",
       " 'is_mtia',\n",
       " 'is_neg',\n",
       " 'is_nested',\n",
       " 'is_nonzero',\n",
       " 'is_pinned',\n",
       " 'is_quantized',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'is_sparse_csr',\n",
       " 'is_vulkan',\n",
       " 'is_xla',\n",
       " 'is_xpu',\n",
       " 'isclose',\n",
       " 'isfinite',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'istft',\n",
       " 'item',\n",
       " 'itemsize',\n",
       " 'kron',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'lcm',\n",
       " 'lcm_',\n",
       " 'ldexp',\n",
       " 'ldexp_',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'less',\n",
       " 'less_',\n",
       " 'less_equal',\n",
       " 'less_equal_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logcumsumexp',\n",
       " 'logdet',\n",
       " 'logical_and',\n",
       " 'logical_and_',\n",
       " 'logical_not',\n",
       " 'logical_not_',\n",
       " 'logical_or',\n",
       " 'logical_or_',\n",
       " 'logical_xor',\n",
       " 'logical_xor_',\n",
       " 'logit',\n",
       " 'logit_',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstsq',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'mH',\n",
       " 'mT',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_exp',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'module_load',\n",
       " 'moveaxis',\n",
       " 'movedim',\n",
       " 'msort',\n",
       " 'mtia',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'multiply',\n",
       " 'multiply_',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'names',\n",
       " 'nan_to_num',\n",
       " 'nan_to_num_',\n",
       " 'nanmean',\n",
       " 'nanmedian',\n",
       " 'nanquantile',\n",
       " 'nansum',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'nbytes',\n",
       " 'ndim',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'negative',\n",
       " 'negative_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_empty_strided',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nextafter',\n",
       " 'nextafter_',\n",
       " 'nonzero',\n",
       " 'nonzero_static',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'not_equal',\n",
       " 'not_equal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'outer',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'positive',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'put',\n",
       " 'put_',\n",
       " 'q_per_channel_axis',\n",
       " 'q_per_channel_scales',\n",
       " 'q_per_channel_zero_points',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qr',\n",
       " 'qscheme',\n",
       " 'quantile',\n",
       " 'rad2deg',\n",
       " 'rad2deg_',\n",
       " 'random_',\n",
       " 'ravel',\n",
       " 'real',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'refine_names',\n",
       " 'register_hook',\n",
       " 'register_post_accumulate_grad_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'rename',\n",
       " 'rename_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'repeat_interleave',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'resize_as_sparse_',\n",
       " 'resolve_conj',\n",
       " 'resolve_neg',\n",
       " 'retain_grad',\n",
       " 'retains_grad',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'row_indices',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'scatter_reduce',\n",
       " 'scatter_reduce_',\n",
       " 'select',\n",
       " 'select_scatter',\n",
       " 'set_',\n",
       " 'sgn',\n",
       " 'sgn_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'signbit',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinc',\n",
       " 'sinc_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slice_inverse',\n",
       " 'slice_scatter',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'square',\n",
       " 'square_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'subtract',\n",
       " 'subtract_',\n",
       " 'sum',\n",
       " 'sum_to_size',\n",
       " 'svd',\n",
       " 'swapaxes',\n",
       " 'swapaxes_',\n",
       " 'swapdims',\n",
       " 'swapdims_',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'take_along_dim',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'tensor_split',\n",
       " 'tile',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_mkldnn',\n",
       " 'to_padded_tensor',\n",
       " 'to_sparse',\n",
       " 'to_sparse_bsc',\n",
       " 'to_sparse_bsr',\n",
       " 'to_sparse_coo',\n",
       " 'to_sparse_csc',\n",
       " 'to_sparse_csr',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'true_divide',\n",
       " 'true_divide_',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unflatten',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsafe_chunk',\n",
       " 'unsafe_split',\n",
       " 'unsafe_split_with_sizes',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'untyped_storage',\n",
       " 'values',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'vsplit',\n",
       " 'where',\n",
       " 'xlogy',\n",
       " 'xlogy_',\n",
       " 'xpu',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(cfloat.imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "builtin_dtypes = [int, float, complex, bool,str, bytes,list,tuple, set, dict]\n",
    "\n",
    "\n",
    "\n",
    "h(\"Built-in types\")\n",
    "\n",
    "_pytypes = collect_typeclass_rows(builtin_dtypes)\n",
    "\n",
    "print_typeclass_rows(_pytypes, \"type\", \"label\", \"to_string\", \"ocltype\", \"ctype\", \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7f9db222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('a', '<i4'), ('b', '<f8')], align=True)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struc =  dtypes.struct('Pair', a=dtypes.int32, b=dtypes.float64)\n",
    "struc.as_numpy_dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6a4628bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dace.dtypes.LP_c_float'>\n"
     ]
    }
   ],
   "source": [
    "p = dtypes.pointer(dtypes.float32)\n",
    "print(p.as_ctypes())\n",
    "# p.as_numpy_dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ab3ece22",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dace.dtypes' has no attribute 'LP_c_float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[160]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28misinstance\u001b[39m(p.as_ctypes(), \u001b[43mdace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLP_c_float\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'dace.dtypes' has no attribute 'LP_c_float'"
     ]
    }
   ],
   "source": [
    "isinstance(p.as_ctypes(), dace.dtypes.LP_c_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "365be734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'struct',\n",
       " 'name': 'Pair',\n",
       " 'data': [('a', 'int32'), ('b', 'float64')],\n",
       " 'length': [],\n",
       " 'bytes': 12}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struc.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "19d6afab",
   "metadata": {},
   "outputs": [
    {
     "ename": "CudaSupportError",
     "evalue": "Error at driver init: \n\nCUDA driver library cannot be found.\nIf you are sure that a CUDA driver is installed,\ntry setting environment variable NUMBA_CUDA_DRIVER\nwith the file path of the CUDA driver shared library.\n:",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCudaSupportError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[165]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumba\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m a = np.arange(\u001b[32m8\u001b[39m, dtype=np.float32)\n\u001b[32m      5\u001b[39m da = numba.cuda.to_device(a) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/dace/lib/python3.13/site-packages/numba/cuda/cudadrv/devices.py:43\u001b[39m, in \u001b[36m_DeviceList.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlst\u001b[49m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/dace/lib/python3.13/site-packages/numba/cuda/cudadrv/devices.py:26\u001b[39m, in \u001b[36m_DeviceList.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# First time looking at \"lst\" attribute.\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mlst\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     24\u001b[39m         \u001b[38;5;66;03m# Device list is not initialized.\u001b[39;00m\n\u001b[32m     25\u001b[39m         \u001b[38;5;66;03m# Query all CUDA devices.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         numdev = \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_device_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m         gpus = [_DeviceContextManager(driver.get_device(devid))\n\u001b[32m     28\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m devid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numdev)]\n\u001b[32m     29\u001b[39m         \u001b[38;5;66;03m# Define \"lst\" to avoid re-initialization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/dace/lib/python3.13/site-packages/numba/cuda/cudadrv/driver.py:424\u001b[39m, in \u001b[36mDriver.get_device_count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    421\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cuDeviceGetCount()\n\u001b[32m    423\u001b[39m count = c_int()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcuDeviceGetCount\u001b[49m(byref(count))\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m count.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/dace/lib/python3.13/site-packages/numba/cuda/cudadrv/driver.py:295\u001b[39m, in \u001b[36mDriver.__getattr__\u001b[39m\u001b[34m(self, fname)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m.ensure_initialized()\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.initialization_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CudaSupportError(\u001b[33m\"\u001b[39m\u001b[33mError at driver init: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m %\n\u001b[32m    296\u001b[39m                            \u001b[38;5;28mself\u001b[39m.initialization_error)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m USE_NV_BINDING:\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cuda_python_wrap_fn(fname)\n",
      "\u001b[31mCudaSupportError\u001b[39m: Error at driver init: \n\nCUDA driver library cannot be found.\nIf you are sure that a CUDA driver is installed,\ntry setting environment variable NUMBA_CUDA_DRIVER\nwith the file path of the CUDA driver shared library.\n:"
     ]
    }
   ],
   "source": [
    "import numba.cuda\n",
    "import numba\n",
    "print(numba.cuda.gpus)\n",
    "a = np.arange(8, dtype=np.float32)\n",
    "da = numba.cuda.to_device(a) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "731f0da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================\n",
      " create_datadescriptor (canonicalization examples)\n",
      "========================================================================\n",
      " - Python int 7          : Scalar (dtype=int64_t, shape=(1,))\n",
      " - numpy int32(5)        : Scalar (dtype=int, shape=(1,))\n",
      " - dtypes.int32          : Scalar (dtype=int, shape=(1,))\n",
      " - None                  : Scalar (dtype=void, shape=(1,))\n",
      "\n",
      "========================================================================\n",
      " Declarative type-spec syntax\n",
      "========================================================================\n",
      " - spec expression       : dace.float32[4,8]\n",
      " - result repr           : Array (dtype=float, shape=(4, 8))\n",
      "\n",
      "Note: composite types and descriptors are different concepts — typeclasses describe element semantics; descriptors describe containers (shape, strides, storage).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "h(\"create_datadescriptor (canonicalization examples)\")\n",
    "examples = [\n",
    "    (\"Python int 7\", 7),\n",
    "    (\"numpy int32(5)\", np.int32(5)),\n",
    "    (\"dtypes.int32\", dtypes.int32),\n",
    "    (\"None\", None),\n",
    "]\n",
    "for name, ex in examples:\n",
    "    try:\n",
    "        desc = data.create_datadescriptor(ex)\n",
    "        kv(name, f\"{type(desc).__name__} (dtype={getattr(desc,'dtype',None)}, shape={getattr(desc,'shape',None)})\")\n",
    "    except Exception as e:\n",
    "        kv(name, f\"ERROR: {e}\")\n",
    "        \n",
    "        \n",
    "h(\"Declarative type-spec syntax\")\n",
    "spec = dace.float32[4, 8]\n",
    "kv(\"spec expression\", \"dace.float32[4,8]\")\n",
    "kv(\"result repr\", spec)\n",
    "\n",
    "print(\"\\nNote: composite types and descriptors are different concepts — typeclasses describe element semantics; descriptors describe containers (shape, strides, storage).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9618d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typeclass example and bridges:\n",
      "  dace.float32 -> float\n",
      "  as_numpy_dtype: float32\n",
      "  as_ctypes: <class 'ctypes.c_float'>\n",
      "\n",
      "Created descriptor: Array (dtype=float, shape=(2, 3))\n",
      "Runtime array shape/dtype: (2, 3) float32\n",
      "Runtime array contents:\n",
      " [[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "\n",
      "SDFG arrays:\n",
      "  A -> Array (dtype=float, shape=(2, 3))\n",
      "\n",
      "Array dtype in SDFG: float\n"
     ]
    }
   ],
   "source": [
    "# Live example: typeclass -> descriptor -> runtime array\n",
    "import dace\n",
    "import numpy as np\n",
    "from dace import dtypes, data\n",
    "\n",
    "print('Typeclass example and bridges:')\n",
    "print('  dace.float32 ->', dtypes.float32)\n",
    "print('  as_numpy_dtype:', dtypes.float32.as_numpy_dtype())\n",
    "print('  as_ctypes:', dtypes.float32.as_ctypes())\n",
    "\n",
    "# Create a container descriptor explicitly\n",
    "desc = data.Array(dtype=dtypes.float32, shape=[2, 3], strides=None)\n",
    "print('\\nCreated descriptor:', desc)\n",
    "\n",
    "# Make a runtime NumPy array from the descriptor (helper)\n",
    "arr = data.make_array_from_descriptor(desc)\n",
    "print('Runtime array shape/dtype:', arr.shape, arr.dtype)\n",
    "arr[:] = np.arange(arr.size).reshape(arr.shape)\n",
    "print('Runtime array contents:\\n', arr)\n",
    "\n",
    "# Small SDFG example that uses a descriptor via sdfg.add_array\n",
    "sdfg = dace.SDFG('example_part1')\n",
    "sdfg.add_array('A', [2,3], dtypes.float32)\n",
    "print('\\nSDFG arrays:')\n",
    "for name, a in sdfg.arrays.items():\n",
    "    print(' ', name, '->', a)\n",
    "\n",
    "# Show that the SDFG array descriptor references a typeclass\n",
    "print('\\nArray dtype in SDFG:', sdfg.arrays['A'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, dace, numpy as np\n",
    "\n",
    "# From tf.DType:\n",
    "tc = dace.dtypes.typeclass(tf.float32.as_numpy_dtype)\n",
    "\n",
    "# From a tensor:\n",
    "np_dtype = tf.constant(0, dtype=tf.float32).dtype.as_numpy_dtype\n",
    "tc = dace.dtypes.typeclass(np_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817ad28b",
   "metadata": {},
   "source": [
    "### Part 1 — Types deep dive (expanded)\n",
    "\n",
    "This section expands the quick overview with concrete responsibilities, small mental models, and pointers to the code that implements the behavior.\n",
    "\n",
    "Key responsibilities (short):\n",
    "\n",
    "- Element types (`dace.dtypes.typeclass`) wrap NumPy dtypes so they can be used declaratively and mapped to backends. They expose helpers such as `as_numpy_dtype()` and `as_ctypes()` for interop.\n",
    "- Composite types (vectors, pointers, structs) are built on the same `typeclass` API but are *not* NumPy dtypes; they let DaCe express vector types and pointers at compile time.\n",
    "- Container descriptors (`dace.data.Array`, `dace.data.Scalar`, `dace.data.Structure`, ...) describe memory containers and are used by the SDFG IR. They carry shape, strides, storage, lifetime, and dtype (a `typeclass`).\n",
    "\n",
    "Where to look in the codebase:\n",
    "\n",
    "- `dace/dace/dtypes.py` — the `typeclass`, `vector`, `pointer`, `struct`, `dtype_to_typeclass`, `is_array` helpers, and platform enums like `StorageType`/`ScheduleType`.\n",
    "- `dace/dace/data.py` — `create_datadescriptor()`, data descriptor classes (`Array`, `Scalar`), and `make_array_from_descriptor()` / `make_reference_from_descriptor()` helpers.\n",
    "\n",
    "Why this separation matters:\n",
    "\n",
    "- Typeclasses define the element semantics and codegen mapping (how to spell the type in C/CUDA/OpenCL), while descriptors tell the compiler where the memory lives and how it is laid out. This separation enables aggressive data-centric optimizations without conflating runtime buffers with IR descriptors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61308d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primitive typeclasses:\n",
      "  dace.float32 -> float\n",
      "  numpy bridge: float32\n",
      "  ctypes bridge: <class 'ctypes.c_float'>\n",
      "7:  Scalar (dtype=int64_t)\n",
      "(np.int32(5):  Scalar (dtype=int)\n",
      "dtypes.int32:  Scalar (dtype=int)\n",
      "Scalar (dtype=void)\n",
      "\n",
      "Declarative spec example: dane.float32[4,8] -> Array (dtype=float, shape=(4, 8))\n"
     ]
    }
   ],
   "source": [
    "# Typeclass examples and composite types\n",
    "import dace\n",
    "from dace import dtypes\n",
    "\n",
    "print('Primitive typeclasses:')\n",
    "print('  dace.float32 ->', dace.float32)\n",
    "print('  numpy bridge:', dace.float32.as_numpy_dtype())\n",
    "print('  ctypes bridge:', dace.float32.as_ctypes())\n",
    "\n",
    "\n",
    "print(\"7: \", data.create_datadescriptor(7))                 \n",
    "print(\"(np.int32(5): \", data.create_datadescriptor(np.int32(5)))       # Scalar(dtype=dace.int32)\n",
    "print(\"dtypes.int32: \", data.create_datadescriptor(dtypes.int32))      # Scalar(dtype=dace.int32)  (typeclass -> Scalar)\n",
    "print(data.create_datadescriptor(None))              # Scalar(pointer(void))  (represents void* / NoneType)\n",
    "# Declaration-style usage\n",
    "spec = dace.float32[4, 8]\n",
    "print('\\nDeclarative spec example: dane.float32[4,8] ->', spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fa967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Composite types\n",
    "vec4 = dace.vector(dace.float32, 4)\n",
    "ptr_to_float = dace.pointer(dace.float32)\n",
    "mystruct = dace.struct(x=dace.int32, y=dace.float64)\n",
    "print('\\nComposite types:')\n",
    "print('  vector4:', vec4)\n",
    "print('  pointer:', ptr_to_float)\n",
    "print('  struct:', mystruct)\n",
    "\n",
    "# Typeclass to numpy dtype mapping\n",
    "print('\\nTypeclass -> numpy dtype')\n",
    "print('  dtype of vec4 base:', vec4.dtype if hasattr(vec4, 'dtype') else getattr(vec4, 'type', None))\n",
    "print('  as_numpy_dtype for float32:', dace.float32.as_numpy_dtype())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699493cb",
   "metadata": {},
   "source": [
    "\n",
    "- `dace.dtypes` is DaCe's primitive/typeclass layer: `dace.float32`, `dace.int32`, plus composite helpers like `dace.vector`, `dace.pointer`, and `dace.struct`. These objects wrap NumPy dtypes and expose helpers such as `as_numpy_dtype()` and `as_ctypes()`.\n",
    "\n",
    "- `dace.data` contains the *container descriptors* used by the IR: `Array`, `Scalar`, `Stream`, `Structure`, `Tensor` (sparse). These describe memory (shape, layout, storage) and are *not* runtime arrays.\n",
    "\n",
    "This part shows the key concepts and small examples so you can see how DaCe represents element types and how containers are canonicalized from common Python objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf762b7c",
   "metadata": {},
   "source": [
    "### Behavior by input kind\n",
    "1. Objects that already are DaCe descriptors / Custom adapter API\n",
    "    - If `obj` is an instance of `dace.data.Data`, `create_datadescriptor` returns it unchanged.\n",
    "    - If `obj` has a __descriptor__() method or a `.descriptor` attribute, that descriptor is returned. This is the recommended extension point for custom types.\n",
    "  \n",
    "2. Torch tensors\n",
    "    - Special-cased: detected by module/class name checks (type module \"torch\" and class name \"Tensor\"). The function maps torch dtypes -> DaCe typeclasses and sets `storage` to GPU when `obj.device.type == 'cuda'`.\n",
    "\n",
    "3. Objects with array interfaces (NumPy/CuPy/array protocol)\n",
    "\n",
    "    - If dtypes.is_array(obj) is True and the object exposes __array_interface__ or __cuda_array_interface__, the code reads typestr, shape, strides and constructs a dace.data.Array. If the interface shape is empty (0-d), it may be treated as a `Scalar`.\n",
    "\n",
    "4. Python lists / tuples\n",
    "\n",
    "    - `create_datadescriptor` casts them to a NumPy array first (`obj = numpy.array(obj)`) and then follows the array-path above — so lists/tuples canonicalize to a `dace.data.Array `with NumPy-inferred dtype/shape/strides.\n",
    "\n",
    "5. CuPy (ndarray) special-case\n",
    "\n",
    "    - If the object originates from cupy (module/class check), it's handled similarly to NumPy arrays, with storage set to GPU.\n",
    "\n",
    "6. Scalars / numeric types\n",
    "    a. symbolic.issymbolic(...) → Scalar(symbolic.symtype(obj)).\n",
    "    b. If `obj` is an instance of dace.dtypes.typeclass → Scalar(obj).\n",
    "    c. If `obj` is int, float, complex, bool, None, or a Python numeric instance (Number, numpy.number, numpy.bool_), create_datadescriptor returns a Scalar whose dtype is dtypes.typeclass(type(obj)) (i.e., DaCe's typeclass built from that Python/numpy type).\n",
    "    d. If obj is a type and is a NumPy number subclass, it's wrapped into a Scalar(dtypes.typeclass(obj)).\n",
    "\n",
    "7. Strings and callables\n",
    "\n",
    "    a. `str` →` Scalar(dace.string)`.\n",
    "    b. `callable(obj)` → returns a `Scalar(dtypes.callback(None))` (a callback descriptor).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1691498b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophieblock/dev_packages/dace/dace/sdfg/sdfg.py:2410: UserWarning: SDFG 'simple_constant_conversion' is already loaded by another object, recompiling under a different name 'simple_constant_conversion_0'.\n",
      "  warnings.warn(f\"SDFG '{self.name}' is already loaded by another object, recompiling under a different \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.dtype: float64, type: <class 'numpy.dtypes.Float64DType'>\n",
      "A[0]: 0.0\n"
     ]
    }
   ],
   "source": [
    "@dace.program\n",
    "def simple_constant_conversion():\n",
    "    return dace.float64(0)\n",
    "\n",
    "A = simple_constant_conversion()\n",
    "print(f\"A.dtype: {A.dtype}, type: {type(A.dtype)}\")\n",
    "print(f\"A[0]: {A[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300b5f4",
   "metadata": {},
   "source": [
    "If the interface shape is empty (0-d), it may be treated as a `Scalar`\n",
    "\n",
    "ex:\n",
    "```python\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = np.arange(12, dtype=np.int32).reshape(3, 4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa11de",
   "metadata": {},
   "source": [
    "*Python builtin sequence types*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ccb64e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Package: builtins.list, is_array=False\n",
      "  Descriptor type: Array, dtype=int64_t, shape=(3,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lst = [1,2,3]\n",
    "show_desc(lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a607b7a",
   "metadata": {},
   "source": [
    "#### Torch tensors\n",
    "Special-cased: detected by module/class name checks (type module \"torch\" and class name \"Tensor\"). The function maps torch dtypes -> DaCe typeclasses and sets `storage` to GPU when `obj.device.type == 'cuda'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t = torch.zeros((2,3))\n",
    "show_desc(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe47f5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94822392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Package: __main__.c_float_Array_6, is_array=True\n",
      "Could not create descriptor for c_float_Array_6: Could not create a DaCe data descriptor from object <__main__.c_float_Array_6 object at 0x12160eed0>. If this is a custom object, consider creating a `__descriptor__` adaptor method to the type hint or object itself.\n"
     ]
    }
   ],
   "source": [
    "# ctypes array\n",
    "import ctypes\n",
    "carr = (ctypes.c_float*6)(*range(6))\n",
    "show_desc(carr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c94829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array (dtype=float, shape=(2, 3))\n",
      "<class 'dace.data.Array'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spec = dace.float32[2, 3]\n",
    "print(spec)\n",
    "print(type(spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d37a30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61fd4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4868a87c",
   "metadata": {},
   "source": [
    "## Part 2: Exploring Existing LibraryNodes\n",
    "\n",
    "Let's start by examining what LibraryNodes are already available in DaCe and what information they expose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6177224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MatMul LibraryNode Analysis ===\n",
      "Class: <class 'dace.libraries.blas.nodes.matmul.MatMul'>\n",
      "Base classes: (<class 'dace.sdfg.nodes.LibraryNode'>,)\n",
      "Is a LibraryNode: True\n",
      "\n",
      "Instance name: example_matmul\n",
      "Instance label: example_matmul\n",
      "Alpha: 1\n",
      "Beta: 0\n",
      "Implementation: None\n",
      "Schedule: ScheduleType.Default\n",
      "\n",
      "Input connectors: {'_a': void, '_b': void}\n",
      "Output connectors: {'_c': void}\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the BLAS MatMul library node as an example\n",
    "from dace.libraries.blas.nodes.matmul import MatMul\n",
    "\n",
    "print(\"=== MatMul LibraryNode Analysis ===\")\n",
    "print(f\"Class: {MatMul}\")\n",
    "print(f\"Base classes: {MatMul.__bases__}\")\n",
    "print(f\"Is a LibraryNode: {issubclass(MatMul, dace.sdfg.nodes.LibraryNode)}\")\n",
    "\n",
    "# Create an instance to inspect\n",
    "mm = MatMul(\"example_matmul\")\n",
    "print(f\"\\nInstance name: {mm.name}\")\n",
    "print(f\"Instance label: {mm.label}\")\n",
    "print(f\"Alpha: {mm.alpha}\")\n",
    "print(f\"Beta: {mm.beta}\")\n",
    "print(f\"Implementation: {mm.implementation}\")\n",
    "print(f\"Schedule: {mm.schedule}\")\n",
    "\n",
    "# Check input/output interface\n",
    "print(f\"\\nInput connectors: {mm.in_connectors}\")\n",
    "print(f\"Output connectors: {mm.out_connectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bfce554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Compile-time Facts (Properties) ===\n",
      "MatMul instance properties:\n",
      "  alpha: 1\n",
      "  beta: 0\n",
      "  debuginfo: <dace.dtypes.DebugInfo object at 0x125a80d10>\n",
      "  environments: frozenset()\n",
      "  guid: f3f8ff1b-a526-46f8-9ef1-5a01893a4b41\n",
      "  implementation: None\n",
      "  in_connectors: {'_a': void, '_b': void}\n",
      "  label: test_matmul\n",
      "  location: {}\n",
      "  name: test_matmul\n",
      "  out_connectors: {'_c': void}\n",
      "  schedule: ScheduleType.Default\n",
      "\n",
      "MatMul class attributes:\n",
      "  alpha: <dace.properties.Property object at 0x125e96890>\n",
      "  beta: <dace.properties.Property object at 0x125e966d0>\n",
      "  debuginfo: <dace.properties.DebugInfoProperty object at 0x125aa4c50>\n",
      "  environments: <dace.properties.SetProperty object at 0x125a0a3c0>\n",
      "  guid: <dace.properties.Property object at 0x125a2eb30>\n",
      "  implementation: <dace.properties.LibraryImplementationProperty object at 0x125a0aa50>\n",
      "  in_connectors: <dace.properties.DictProperty object at 0x125853390>\n",
      "  label: <dace.properties.Property object at 0x125a2f310>\n",
      "  location: <dace.properties.DictProperty object at 0x125a25350>\n",
      "  name: <dace.properties.Property object at 0x125ada5f0>\n",
      "  out_connectors: <dace.properties.DictProperty object at 0x125853b10>\n",
      "  schedule: <dace.properties.EnumProperty object at 0x125aa4b50>\n",
      "\n",
      "=== Available Implementations ===\n",
      "  specialize: <class 'dace.libraries.blas.nodes.matmul.SpecializeMatMul'>\n",
      "\n",
      "Default implementation: specialize\n",
      "\n",
      "=== Library Registration ===\n",
      "Library name: blas\n",
      "Is library node: True\n"
     ]
    }
   ],
   "source": [
    "# Check what \"facts\" are established at compile-time\n",
    "print(f\"\\n=== Compile-time Facts (Properties) ===\")\n",
    "\n",
    "# Inspect MatMul properties by examining the instance\n",
    "mm_instance = MatMul(\"test_matmul\")\n",
    "print(f\"MatMul instance properties:\")\n",
    "for prop_name in dir(mm_instance):\n",
    "    if not prop_name.startswith('_') and isinstance(getattr(MatMul, prop_name, None), dace.properties.Property):\n",
    "        prop_val = getattr(mm_instance, prop_name)\n",
    "        print(f\"  {prop_name}: {prop_val}\")\n",
    "\n",
    "# Let's also check class attributes\n",
    "print(f\"\\nMatMul class attributes:\")\n",
    "for attr_name in dir(MatMul):\n",
    "    if not attr_name.startswith('_') and not callable(getattr(MatMul, attr_name)):\n",
    "        attr_val = getattr(MatMul, attr_name)\n",
    "        if isinstance(attr_val, dace.properties.Property):\n",
    "            print(f\"  {attr_name}: {attr_val}\")\n",
    "\n",
    "# Check available implementations\n",
    "print(f\"\\n=== Available Implementations ===\")\n",
    "for impl_name, impl_class in MatMul.implementations.items():\n",
    "    print(f\"  {impl_name}: {impl_class}\")\n",
    "\n",
    "print(f\"\\nDefault implementation: {MatMul.default_implementation}\")\n",
    "\n",
    "# Check library registration\n",
    "print(f\"\\n=== Library Registration ===\")\n",
    "print(f\"Library name: {getattr(MatMul, '_dace_library_name', 'Not registered')}\")\n",
    "print(f\"Is library node: {getattr(MatMul, '_dace_library_node', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ae31e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LibraryNode Instance Analysis ===\n",
      "Node name: example_gemm\n",
      "Node label: example_gemm\n",
      "Current implementation: None\n",
      "\n",
      "=== Semantic Properties (Compile-time Facts) ===\n",
      "Alpha coefficient: 2.0\n",
      "Beta coefficient: 0\n",
      "Transpose A: True\n",
      "Transpose B: False\n",
      "\n",
      "=== Connector Information ===\n",
      "Input connectors: {'_a': void, '_b': void}\n",
      "Output connectors: {'_c': void}\n",
      "\n",
      "=== What the node 'knows' about itself ===\n",
      "Has side effects: False\n",
      "Free symbols: set()\n",
      "\n",
      "=== Using Gemm in an SDFG ===\n",
      "SDFG with GEMM node created: 4 nodes, 4 edges\n",
      "GEMM performs: C = 2.0*Aᵀ*B + 0*C\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the GEMM (GEneral Matrix Multiplication) library node\n",
    "from dace.libraries.blas.nodes.gemm import Gemm\n",
    "\n",
    "# Create an instance with specific compile-time facts\n",
    "gemm_node = Gemm(\"example_gemm\", \n",
    "                  alpha=2.0,  # Scalar coefficient for AB\n",
    "                  beta=0,   # Scalar coefficient for C\n",
    "                  transA=True,  # Whether to transpose A\n",
    "                  transB=False)  # Whether to transpose B\n",
    "\n",
    "print(\"=== LibraryNode Instance Analysis ===\")\n",
    "print(f\"Node name: {gemm_node.name}\")\n",
    "print(f\"Node label: {gemm_node.label}\")\n",
    "print(f\"Current implementation: {gemm_node.implementation}\")\n",
    "\n",
    "print(f\"\\n=== Semantic Properties (Compile-time Facts) ===\")\n",
    "print(f\"Alpha coefficient: {gemm_node.alpha}\")\n",
    "print(f\"Beta coefficient: {gemm_node.beta}\")\n",
    "print(f\"Transpose A: {gemm_node.transA}\")\n",
    "print(f\"Transpose B: {gemm_node.transB}\")\n",
    "\n",
    "print(f\"\\n=== Connector Information ===\")\n",
    "print(f\"Input connectors: {gemm_node.in_connectors}\")\n",
    "print(f\"Output connectors: {gemm_node.out_connectors}\")\n",
    "\n",
    "print(f\"\\n=== What the node 'knows' about itself ===\")\n",
    "print(f\"Has side effects: {gemm_node.has_side_effects}\")\n",
    "print(f\"Free symbols: {gemm_node.free_symbols}\")  # Symbolic dependencies\n",
    "\n",
    "# Create a simple SDFG using this node\n",
    "print(f\"\\n=== Using Gemm in an SDFG ===\")\n",
    "sdfg = SDFG(\"gemm_example\")\n",
    "state = sdfg.add_state()\n",
    "\n",
    "# Add arrays with specific sizes\n",
    "M, N, K = 3, 4, 5\n",
    "sdfg.add_array(\"A\", [M, K], dace.float32)\n",
    "sdfg.add_array(\"B\", [K, N], dace.float32)\n",
    "sdfg.add_array(\"C\", [M, N], dace.float32)\n",
    "\n",
    "# Add access nodes\n",
    "a_access = state.add_read(\"A\")\n",
    "b_access = state.add_read(\"B\")\n",
    "c_access = state.add_access(\"C\")  # Both read and write\n",
    "\n",
    "# Connect to GEMM node (C = alpha*A*B + beta*C)\n",
    "state.add_edge(a_access, None, gemm_node, \"_a\", dace.Memlet(\"A[0:M, 0:K]\"))\n",
    "state.add_edge(b_access, None, gemm_node, \"_b\", dace.Memlet(\"B[0:K, 0:N]\"))\n",
    "state.add_edge(c_access, None, gemm_node, \"_c\", dace.Memlet(\"C[0:M, 0:N]\"))\n",
    "state.add_edge(gemm_node, \"_o\", c_access, None, dace.Memlet(\"C[0:M, 0:N]\"))\n",
    "\n",
    "print(f\"SDFG with GEMM node created: {len(state.nodes())} nodes, {len(state.edges())} edges\")\n",
    "print(f\"GEMM performs: C = {gemm_node.alpha}*A{'ᵀ' if gemm_node.transA else ''}*B{'ᵀ' if gemm_node.transB else ''} + {gemm_node.beta}*C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7297283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== With beta=0 ===\n",
      "Input connectors: {'_a': void, '_b': void}\n",
      "Output connectors: {'_c': void}\n",
      "\n",
      "=== With beta=1 ===\n",
      "Input connectors: {'_a': void, '_c': void, '_b': void}\n",
      "Output connectors: {'_c': void}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Code to inspect Gemm connectors\n",
    "gemm = Gemm(\"example_gemm\", beta=0)\n",
    "print(\"=== With beta=0 ===\")\n",
    "print(\"Input connectors:\", gemm.in_connectors)  # Should be {'_a', '_b'}\n",
    "print(\"Output connectors:\", gemm.out_connectors)  # Should be {'_c'}\n",
    "\n",
    "gemm_with_c = Gemm(\"example_gemm_with_c\", beta=1)\n",
    "print(\"\\n=== With beta=1 ===\")\n",
    "print(\"Input connectors:\", gemm_with_c.in_connectors)  # Should be {'_a', '_b', '_c'}\n",
    "print(\"Output connectors:\", gemm_with_c.out_connectors)  # Should be {'_c'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f24e51",
   "metadata": {},
   "source": [
    "## Part 2: Creating a Custom LibraryNode\n",
    "\n",
    "Now let's create our own LibraryNode to understand how the `@dace.library.node` decorator establishes semantic contracts. We'll define a custom Vector Addition node as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e57c4ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== After @dace.library.node decoration ===\n",
      "_dace_library_node: True\n",
      "Implementations dictionary: {}\n",
      "Default implementation: None\n",
      "\n",
      "Declared properties:\n",
      "  debuginfo: <dace.properties.DebugInfoProperty object at 0x125aa4c50>\n",
      "  environments: <dace.properties.SetProperty object at 0x125a0a3c0>\n",
      "  guid: <dace.properties.Property object at 0x125a2eb30>\n",
      "  implementation: <dace.properties.LibraryImplementationProperty object at 0x125a0aa50>\n",
      "  in_connectors: <dace.properties.DictProperty object at 0x125853390>\n",
      "  label: <dace.properties.Property object at 0x125a2f310>\n",
      "  location: <dace.properties.DictProperty object at 0x125a25350>\n",
      "  n: <dace.properties.SymbolicProperty object at 0x125e2b4d0>\n",
      "  name: <dace.properties.Property object at 0x125ada5f0>\n",
      "  out_connectors: <dace.properties.DictProperty object at 0x125853b10>\n",
      "  scale_factor: <dace.properties.Property object at 0x127346190>\n",
      "  schedule: <dace.properties.EnumProperty object at 0x125aa4b50>\n",
      "\n",
      "Instance properties:\n",
      "  n = 1024\n",
      "  scale_factor = 2.5\n",
      "  inputs = {'_a': void, '_b': void}\n",
      "  outputs = {'_c': void}\n"
     ]
    }
   ],
   "source": [
    "# Let's create a simple vector addition LibraryNode\n",
    "# This will demonstrate the semantic contract system\n",
    "\n",
    "# Define a custom Library Node class\n",
    "@dace.library.node\n",
    "class VectorAdd(dace.sdfg.nodes.LibraryNode):\n",
    "    \"\"\"\n",
    "    A LibraryNode that represents vector addition: C = A + B\n",
    "    \n",
    "    This demonstrates how LibraryNodes establish semantic contracts:\n",
    "    - What the operation does (vector addition)\n",
    "    - What parameters it needs (vector size, scalar coefficient)\n",
    "    - What data it consumes/produces (two input vectors, one output)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Required: Define possible implementations (initially empty)\n",
    "    implementations = {}\n",
    "    \n",
    "    # Required: Define default implementation (can be None initially)\n",
    "    default_implementation = None\n",
    "    \n",
    "    # Semantic properties (compile-time \"facts\")\n",
    "    n = dace.properties.SymbolicProperty(\n",
    "        desc=\"Vector size\",\n",
    "        default=symbolic.symbol(\"n\")\n",
    "    )\n",
    "    \n",
    "    scale_factor = dace.properties.Property(\n",
    "        dtype=float,\n",
    "        default=1.0,\n",
    "        desc=\"Scalar multiplier for the result\"\n",
    "    )\n",
    "    \n",
    "    def __init__(self, name, n=None, scale_factor=1.0, **kwargs):\n",
    "        # Define the semantic interface: 2 inputs, 1 output\n",
    "        super().__init__(name, \n",
    "                         inputs={\"_a\", \"_b\"},  # Input connectors\n",
    "                         outputs={\"_c\"},       # Output connectors\n",
    "                         **kwargs)\n",
    "        \n",
    "        # Set semantic properties\n",
    "        self.n = n or symbolic.symbol(\"n\")\n",
    "        self.scale_factor = scale_factor\n",
    "    \n",
    "    def validate(self, sdfg, state):\n",
    "        \"\"\"\n",
    "        Semantic validation - ensure the node usage is correct\n",
    "        This is where compile-time \"facts\" are checked\n",
    "        \"\"\"\n",
    "        # Check that we have the right connectors\n",
    "        assert \"_a\" in self.in_connectors, \"VectorAdd requires _a input connector\"\n",
    "        assert \"_b\" in self.in_connectors, \"VectorAdd requires _b input connector\"\n",
    "        assert \"_c\" in self.out_connectors, \"VectorAdd requires _c output connector\"\n",
    "        \n",
    "        # Additional semantic checks could go here\n",
    "        # (e.g., vector sizes match, dtypes are compatible, etc.)\n",
    "        pass\n",
    "\n",
    "# After decoration, let's see what the decorator added\n",
    "print(\"=== After @dace.library.node decoration ===\")\n",
    "print(f\"_dace_library_node: {getattr(VectorAdd, '_dace_library_node', 'Missing')}\")\n",
    "print(f\"Implementations dictionary: {VectorAdd.implementations}\")\n",
    "print(f\"Default implementation: {VectorAdd.default_implementation}\")\n",
    "\n",
    "# Inspect the properties\n",
    "print(f\"\\nDeclared properties:\")\n",
    "for attr_name in dir(VectorAdd):\n",
    "    attr_val = getattr(VectorAdd, attr_name)\n",
    "    if isinstance(attr_val, dace.properties.Property):\n",
    "        print(f\"  {attr_name}: {attr_val}\")\n",
    "\n",
    "# Create an instance and check its properties\n",
    "vector_add = VectorAdd(\"my_vector_add\", n=1024, scale_factor=2.5)\n",
    "print(f\"\\nInstance properties:\")\n",
    "print(f\"  n = {vector_add.n}\")\n",
    "print(f\"  scale_factor = {vector_add.scale_factor}\")\n",
    "print(f\"  inputs = {vector_add.in_connectors}\")\n",
    "print(f\"  outputs = {vector_add.out_connectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6dc71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Compile-time Facts Available to DaCe ===\n",
      "Operation: Vector addition with scaling\n",
      "Semantic interface: 2 inputs → 1 outputs\n",
      "Vector size: 1024\n",
      "Scale factor: 2.5\n",
      "Input connectors: {'_b': void, '_a': void}\n",
      "Output connectors: {'_c': void}\n",
      "\n",
      "=== What DaCe Can Reason About ===\n",
      "Free symbols (symbolic dependencies): set()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VectorAdd' object has no attribute '_dace_properties'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== What DaCe Can Reason About ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFree symbols (symbolic dependencies): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvector_add_node.free_symbols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNode properties: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[43mvector_add_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dace_properties\u001b[49m.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Try to use it in an SDFG (this will fail because no implementation exists yet)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Trying to Use Without Implementation ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'VectorAdd' object has no attribute '_dace_properties'"
     ]
    }
   ],
   "source": [
    "# Create an instance and inspect the compile-time facts\n",
    "vector_add_node = VectorAdd(\"my_vector_add\", n=1024, scale_factor=2.5)\n",
    "\n",
    "print(\"=== Compile-time Facts Available to DaCe ===\")\n",
    "print(f\"Operation: Vector addition with scaling\")\n",
    "print(f\"Semantic interface: {len(vector_add_node.in_connectors)} inputs → {len(vector_add_node.out_connectors)} outputs\")\n",
    "print(f\"Vector size: {vector_add_node.n}\")\n",
    "print(f\"Scale factor: {vector_add_node.scale_factor}\")\n",
    "print(f\"Input connectors: {vector_add_node.in_connectors}\")\n",
    "print(f\"Output connectors: {vector_add_node.out_connectors}\")\n",
    "\n",
    "print(f\"\\n=== What DaCe Can Reason About ===\")\n",
    "print(f\"Free symbols (symbolic dependencies): {vector_add_node.free_symbols}\")\n",
    "\n",
    "# Try to use it in an SDFG (this will fail because no implementation exists yet)\n",
    "print(f\"\\n=== Trying to Use Without Implementation ===\")\n",
    "try:\n",
    "    sdfg = SDFG(\"test_vector_add\")\n",
    "    state = sdfg.add_state()\n",
    "    \n",
    "    # Add arrays\n",
    "    sdfg.add_array(\"A\", [1024], dace.float32)\n",
    "    sdfg.add_array(\"B\", [1024], dace.float32) \n",
    "    sdfg.add_array(\"C\", [1024], dace.float32)\n",
    "    \n",
    "    # Add nodes\n",
    "    a_read = state.add_read(\"A\")\n",
    "    b_read = state.add_read(\"B\")\n",
    "    c_write = state.add_write(\"C\")\n",
    "    vector_add = state.add_node(vector_add_node)\n",
    "    \n",
    "    # Add edges\n",
    "    state.add_edge(a_read, None, vector_add, \"_a\", dace.Memlet(\"A[0:1024]\"))\n",
    "    state.add_edge(b_read, None, vector_add, \"_b\", dace.Memlet(\"B[0:1024]\"))\n",
    "    state.add_edge(vector_add, \"_c\", c_write, None, dace.Memlet(\"C[0:1024]\"))\n",
    "    \n",
    "    print(\"SDFG created successfully!\")\n",
    "    print(f\"Trying to compile...\")\n",
    "    # This will fail because we have no implementation\n",
    "    sdfg.validate()\n",
    "    compiled = sdfg.compile()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Expected error: {type(e).__name__}: {e}\")\n",
    "    print(\"This is expected - we need to define an implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b470d",
   "metadata": {},
   "source": [
    "## Part 3: Registering Implementations (Expansions)\n",
    "\n",
    "Now we'll create concrete implementations for our LibraryNode. This is where abstract operations become executable code. Each implementation defines how the operation will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4063bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple CPU implementation using @dace.library.expansion\n",
    "@dace.library.expansion\n",
    "class ExpandVectorAddCPU(ExpandTransformation):\n",
    "    \"\"\"\n",
    "    A concrete implementation of VectorAdd for CPU execution.\n",
    "    \n",
    "    This demonstrates how abstract LibraryNodes get translated into\n",
    "    executable SDFG subgraphs.\n",
    "    \"\"\"\n",
    "    \n",
    "    environments = []  # No special environment requirements for CPU\n",
    "    \n",
    "    @staticmethod\n",
    "    def expansion(node: VectorAdd, parent_state: SDFGState, parent_sdfg: SDFG, **kwargs):\n",
    "        \"\"\"\n",
    "        This method gets called when DaCe needs to expand the LibraryNode\n",
    "        into concrete operations.\n",
    "        \n",
    "        Here's what information is available during expansion:\n",
    "        - node: The LibraryNode instance with all its compile-time facts\n",
    "        - parent_state: The SDFG state containing the node\n",
    "        - parent_sdfg: The parent SDFG for context\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"=== Expansion Context ===\")\n",
    "        print(f\"Expanding node: {node.name}\")\n",
    "        print(f\"Node properties available:\")\n",
    "        print(f\"  - n (vector size): {node.n}\")\n",
    "        print(f\"  - scale_factor: {node.scale_factor}\")\n",
    "        print(f\"  - input connectors: {node.in_connectors}\")\n",
    "        print(f\"  - output connectors: {node.out_connectors}\")\n",
    "        \n",
    "        # Access the memlets/data arrays connected to this node\n",
    "        input_edges = parent_state.in_edges(node)\n",
    "        output_edges = parent_state.out_edges(node)\n",
    "        \n",
    "        print(f\"Input edges: {len(input_edges)}\")\n",
    "        print(f\"Output edges: {len(output_edges)}\")\n",
    "        \n",
    "        # Create the expansion: a simple map with a tasklet\n",
    "        # This replaces the LibraryNode with concrete SDFG operations\n",
    "        \n",
    "        # Create a map over the vector elements\n",
    "        map_entry, map_exit = parent_state.add_map(\n",
    "            'vector_add_map',\n",
    "            dict(i=f'0:{node.n}')\n",
    "        )\n",
    "        \n",
    "        # Create a tasklet that does the actual computation\n",
    "        tasklet = parent_state.add_tasklet(\n",
    "            'add_task',\n",
    "            inputs={'a_in', 'b_in'},\n",
    "            outputs={'c_out'},\n",
    "            code=f'c_out = ({node.scale_factor} * (a_in + b_in))'\n",
    "        )\n",
    "        \n",
    "        # Get input and output array names from edges\n",
    "        a_array = None\n",
    "        b_array = None\n",
    "        c_array = None\n",
    "        \n",
    "        for e in input_edges:\n",
    "            if e.dst_conn == '_a':\n",
    "                a_array = e.data.data\n",
    "            elif e.dst_conn == '_b':\n",
    "                b_array = e.data.data\n",
    "        \n",
    "        for e in output_edges:\n",
    "            if e.src_conn == '_c':\n",
    "                c_array = e.data.data\n",
    "        \n",
    "        # Create read/write nodes within the map\n",
    "        a_read = parent_state.add_read(a_array)\n",
    "        b_read = parent_state.add_read(b_array)\n",
    "        c_write = parent_state.add_write(c_array)\n",
    "        \n",
    "        # Connect everything\n",
    "        parent_state.add_edge(map_entry, None, a_read, None, dace.Memlet())\n",
    "        parent_state.add_edge(map_entry, None, b_read, None, dace.Memlet())\n",
    "        parent_state.add_edge(map_entry, None, tasklet, None, dace.Memlet())\n",
    "        \n",
    "        # Connect reads to tasklet\n",
    "        parent_state.add_edge(a_read, None, tasklet, 'a_in', dace.Memlet(f\"{a_array}[i]\"))\n",
    "        parent_state.add_edge(b_read, None, tasklet, 'b_in', dace.Memlet(f\"{b_array}[i]\"))\n",
    "        \n",
    "        # Connect tasklet to write\n",
    "        parent_state.add_edge(tasklet, 'c_out', c_write, None, dace.Memlet(f\"{c_array}[i]\"))\n",
    "        \n",
    "        # Connect to map exit\n",
    "        parent_state.add_edge(c_write, None, map_exit, None, dace.Memlet())\n",
    "        \n",
    "        # Connect the incoming edges from the LibraryNode to our subgraph\n",
    "        for e in input_edges:\n",
    "            if e.dst_conn == '_a':\n",
    "                parent_state.add_edge(e.src, e.src_conn, a_read, None, e.data)\n",
    "            elif e.dst_conn == '_b':\n",
    "                parent_state.add_edge(e.src, e.src_conn, b_read, None, e.data)\n",
    "        \n",
    "        # Connect the outgoing edges from our subgraph\n",
    "        for e in output_edges:\n",
    "            if e.src_conn == '_c':\n",
    "                parent_state.add_edge(c_write, None, e.dst, e.dst_conn, e.data)\n",
    "        \n",
    "        # Remove the library node from the graph - it's replaced by our implementation\n",
    "        parent_state.remove_node(node)\n",
    "        \n",
    "        print(\"Expansion created: Map + Tasklet for CPU execution\")\n",
    "        return map_entry  # Return the entry point of the expansion\n",
    "\n",
    "# Register this implementation with our LibraryNode\n",
    "VectorAdd.register_implementation('cpu', ExpandVectorAddCPU)\n",
    "VectorAdd.default_implementation = 'cpu'\n",
    "\n",
    "print(f\"Registered 'cpu' implementation for VectorAdd\")\n",
    "print(f\"Available implementations: {list(VectorAdd.implementations.keys())}\")\n",
    "print(f\"Default implementation: {VectorAdd.default_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way: Using the @dace.library.register_expansion decorator\n",
    "# This is more convenient for adding implementations after node definition\n",
    "\n",
    "@dace.library.register_expansion(VectorAdd, 'vectorized')\n",
    "class ExpandVectorAddVectorized(ExpandTransformation):\n",
    "    \"\"\"\n",
    "    An alternative implementation that demonstrates vectorized operations.\n",
    "    \n",
    "    This shows how multiple implementations can target different execution\n",
    "    strategies for the same semantic operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    environments = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def expansion(node: VectorAdd, parent_state: SDFGState, parent_sdfg: SDFG, **kwargs):\n",
    "        \"\"\"\n",
    "        This expansion demonstrates accessing more compile-time information\n",
    "        and creating a different implementation strategy.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"=== Vectorized Expansion ===\")\n",
    "        print(f\"Available compile-time facts during expansion:\")\n",
    "        \n",
    "        # All the semantic properties are available\n",
    "        print(f\"  - Vector size (n): {node.n}\")\n",
    "        print(f\"  - Scale factor: {node.scale_factor}\")\n",
    "        print(f\"  - Node schedule: {node.schedule}\")\n",
    "        print(f\"  - Implementation being used: {node.implementation}\")\n",
    "        \n",
    "        # We can also access the parent SDFG context\n",
    "        print(f\"  - Parent SDFG name: {parent_sdfg.name}\")\n",
    "        print(f\"  - Parent state name: {parent_state.label}\")\n",
    "        \n",
    "        # Find connected arrays to get more type information\n",
    "        in_edges = parent_state.in_edges(node)\n",
    "        out_edges = parent_state.out_edges(node)\n",
    "        \n",
    "        # Get array names and descriptors\n",
    "        a_array = None\n",
    "        b_array = None\n",
    "        c_array = None\n",
    "        \n",
    "        for e in in_edges:\n",
    "            if e.dst_conn == '_a':\n",
    "                a_array = e.data.data\n",
    "                array_desc = parent_sdfg.arrays[a_array]\n",
    "                print(f\"  - Input array '{a_array}': shape={array_desc.shape}, dtype={array_desc.dtype}\")\n",
    "            elif e.dst_conn == '_b':\n",
    "                b_array = e.data.data\n",
    "                array_desc = parent_sdfg.arrays[b_array]\n",
    "                print(f\"  - Input array '{b_array}': shape={array_desc.shape}, dtype={array_desc.dtype}\")\n",
    "        \n",
    "        for e in out_edges:\n",
    "            if e.src_conn == '_c':\n",
    "                c_array = e.data.data\n",
    "                array_desc = parent_sdfg.arrays[c_array]\n",
    "                print(f\"  - Output array '{c_array}': shape={array_desc.shape}, dtype={array_desc.dtype}\")\n",
    "        \n",
    "        # Create a different expansion - single tasklet (vectorized)\n",
    "        # This shows that different implementations can have different internal structures\n",
    "        \n",
    "        # Create a single vectorized tasklet for the whole operation\n",
    "        tasklet = parent_state.add_tasklet(\n",
    "            'vectorized_add',\n",
    "            inputs={'a_vec', 'b_vec'},\n",
    "            outputs={'c_vec'},\n",
    "            code=f'''\n",
    "            for (int i = 0; i < {node.n}; ++i) {{\n",
    "                c_vec[i] = {node.scale_factor} * (a_vec[i] + b_vec[i]);\n",
    "            }}\n",
    "            ''',\n",
    "            language=dace.dtypes.Language.CPP\n",
    "        )\n",
    "        \n",
    "        # Create read/write nodes\n",
    "        a_read = parent_state.add_read(a_array)\n",
    "        b_read = parent_state.add_read(b_array)\n",
    "        c_write = parent_state.add_write(c_array)\n",
    "        \n",
    "        # Connect everything with array memlets instead of scalar ones\n",
    "        parent_state.add_edge(a_read, None, tasklet, 'a_vec', dace.Memlet(f\"{a_array}[0:{node.n}]\"))\n",
    "        parent_state.add_edge(b_read, None, tasklet, 'b_vec', dace.Memlet(f\"{b_array}[0:{node.n}]\"))\n",
    "        parent_state.add_edge(tasklet, 'c_vec', c_write, None, dace.Memlet(f\"{c_array}[0:{node.n}]\"))\n",
    "        \n",
    "        # Connect the incoming edges from the library node\n",
    "        for e in in_edges:\n",
    "            if e.dst_conn == '_a':\n",
    "                parent_state.add_edge(e.src, e.src_conn, a_read, None, e.data)\n",
    "            elif e.dst_conn == '_b':\n",
    "                parent_state.add_edge(e.src, e.src_conn, b_read, None, e.data)\n",
    "        \n",
    "        # Connect the outgoing edges \n",
    "        for e in out_edges:\n",
    "            if e.src_conn == '_c':\n",
    "                parent_state.add_edge(c_write, None, e.dst, e.dst_conn, e.data)\n",
    "        \n",
    "        # Remove the original library node\n",
    "        parent_state.remove_node(node)\n",
    "        \n",
    "        print(\"Created vectorized expansion: Single C++ tasklet with array accesses\")\n",
    "        return tasklet\n",
    "\n",
    "print(f\"Registered 'vectorized' implementation for VectorAdd\")\n",
    "print(f\"Available implementations: {list(VectorAdd.implementations.keys())}\")\n",
    "\n",
    "# Show the implementation registry\n",
    "print(f\"\\n=== Implementation Registry ===\")\n",
    "for name, impl_class in VectorAdd.implementations.items():\n",
    "    print(f\"  {name}: {impl_class}\")\n",
    "    print(f\"    - Environments required: {impl_class.environments}\")\n",
    "    print(f\"    - Associated node: {getattr(impl_class, '_dace_library_node', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c521a0d",
   "metadata": {},
   "source": [
    "## Part 4: Testing the Implementation System\n",
    "\n",
    "Now let's test our LibraryNode with its registered implementations to see the expansion in action. We'll create two instances - one using the default CPU implementation and another explicitly using the vectorized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4652df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test SDFG using our LibraryNode\n",
    "print(\"=== Creating Test SDFG ===\")\n",
    "\n",
    "sdfg = SDFG(\"test_vector_add_library\")\n",
    "\n",
    "# Add arrays\n",
    "N = 8  # Small size for demonstration\n",
    "sdfg.add_array(\"A\", [N], dace.float32)\n",
    "sdfg.add_array(\"B\", [N], dace.float32)\n",
    "sdfg.add_array(\"C\", [N], dace.float32)\n",
    "sdfg.add_array(\"D\", [N], dace.float32)  # For second implementation\n",
    "\n",
    "# Create first state with default implementation\n",
    "state1 = sdfg.add_state(\"cpu_implementation\")\n",
    "\n",
    "# Add nodes for first implementation\n",
    "a_read1 = state1.add_read(\"A\")\n",
    "b_read1 = state1.add_read(\"B\")\n",
    "c_write1 = state1.add_write(\"C\")\n",
    "\n",
    "# Create our LibraryNode instance with specific parameters\n",
    "vector_add1 = VectorAdd(\"test_add\", n=N, scale_factor=2.0)\n",
    "vector_add_node1 = state1.add_node(vector_add1)\n",
    "\n",
    "# Connect the LibraryNode\n",
    "state1.add_edge(a_read1, None, vector_add_node1, \"_a\", dace.Memlet(\"A[0:8]\"))\n",
    "state1.add_edge(b_read1, None, vector_add_node1, \"_b\", dace.Memlet(\"B[0:8]\"))\n",
    "state1.add_edge(vector_add_node1, \"_c\", c_write1, None, dace.Memlet(\"C[0:8]\"))\n",
    "\n",
    "print(f\"State 1 created with LibraryNode: {vector_add_node1.name}\")\n",
    "print(f\"Using implementation: {vector_add_node1.implementation or 'default'}\")\n",
    "\n",
    "# Create second state with vectorized implementation\n",
    "state2 = sdfg.add_state(\"vectorized_implementation\")\n",
    "a_read2 = state2.add_read(\"A\")\n",
    "b_read2 = state2.add_read(\"B\") \n",
    "d_write2 = state2.add_write(\"D\")\n",
    "\n",
    "# Create another instance with explicit implementation\n",
    "vector_add2 = VectorAdd(\"test_add_vec\", n=N, scale_factor=3.0)\n",
    "vector_add2.implementation = 'vectorized'  # Explicitly set implementation\n",
    "vector_add_node2 = state2.add_node(vector_add2)\n",
    "\n",
    "# Connect the second LibraryNode\n",
    "state2.add_edge(a_read2, None, vector_add_node2, \"_a\", dace.Memlet(\"A[0:8]\"))\n",
    "state2.add_edge(b_read2, None, vector_add_node2, \"_b\", dace.Memlet(\"B[0:8]\"))\n",
    "state2.add_edge(vector_add_node2, \"_c\", d_write2, None, dace.Memlet(\"D[0:8]\"))\n",
    "\n",
    "print(f\"State 2 created with LibraryNode: {vector_add_node2.name}\")\n",
    "print(f\"Using implementation: {vector_add_node2.implementation}\")\n",
    "\n",
    "# Add an edge between states (sequential execution)\n",
    "sdfg.add_edge(state1, state2, dace.sdfg.InterstateEdge())\n",
    "\n",
    "# Now expand the LibraryNodes\n",
    "print(f\"\\n=== Expanding LibraryNodes ===\")\n",
    "sdfg.expand_library_nodes()\n",
    "\n",
    "print(f\"\\n=== SDFG After Expansion ===\")\n",
    "print(f\"Number of states: {len(sdfg.states())}\")\n",
    "print(f\"Nodes in state 1: {len(state1.nodes())}\")\n",
    "print(f\"Node types in state 1: {[type(node).__name__ for node in state1.nodes()]}\")\n",
    "print(f\"Nodes in state 2: {len(state2.nodes())}\")\n",
    "print(f\"Node types in state 2: {[type(node).__name__ for node in state2.nodes()]}\")\n",
    "\n",
    "# Run the SDFG with test data\n",
    "print(f\"\\n=== Executing SDFG with Test Data ===\")\n",
    "A_data = np.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=np.float32)\n",
    "B_data = np.array([8, 7, 6, 5, 4, 3, 2, 1], dtype=np.float32)\n",
    "\n",
    "try:\n",
    "    # Compile and run the SDFG\n",
    "    print(\"Compiling SDFG...\")\n",
    "    compiled_sdfg = sdfg.compile()\n",
    "    \n",
    "    print(\"Running SDFG...\")\n",
    "    result = compiled_sdfg(A=A_data, B=B_data)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(\"Input A:\", A_data)\n",
    "    print(\"Input B:\", B_data)\n",
    "    print(\"Output C (CPU impl, scale=2.0):\", result['C'])\n",
    "    print(\"Output D (Vectorized impl, scale=3.0):\", result['D'])\n",
    "    \n",
    "    # Verify results\n",
    "    expected_C = 2.0 * (A_data + B_data)\n",
    "    expected_D = 3.0 * (A_data + B_data)\n",
    "    print(\"\\nVerification:\")\n",
    "    print(\"C matches expected:\", np.allclose(result['C'], expected_C))\n",
    "    print(\"D matches expected:\", np.allclose(result['D'], expected_D))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running SDFG: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1edd4",
   "metadata": {},
   "source": [
    "## Part 5: Summary - Compile-time \"Facts\" in DaCe LibraryNodes\n",
    "\n",
    "Let's summarize what compile-time information is available and how the abstraction system works. The key insight is the separation between \"what\" (abstract operation semantics) and \"how\" (concrete implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d58601",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DaCe LibraryNode System Summary ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. SEMANTIC CONTRACTS (@dace.library.node)\")\n",
    "print(\"   - Defines what an operation does, not how\")\n",
    "print(\"   - Establishes compile-time 'facts' via Properties\")\n",
    "print(\"   - Specifies input/output interface (connectors)\")\n",
    "print(\"   - Enables multiple implementations for same abstraction\")\n",
    "print()\n",
    "\n",
    "print(\"2. COMPILE-TIME FACTS AVAILABLE:\")\n",
    "print(\"   a) Semantic Properties:\")\n",
    "print(\"      - Symbolic parameters (e.g., vector/matrix dimensions)\")\n",
    "print(\"      - Algorithm parameters (e.g., alpha/beta coefficients)\")\n",
    "print(\"      - Configuration flags (e.g., transpose flags)\")\n",
    "print()\n",
    "print(\"   b) Interface Information:\")  \n",
    "print(\"      - Input/output connectors\")\n",
    "print(\"      - Data dependencies\") \n",
    "print(\"      - Memory access patterns\")\n",
    "print()\n",
    "print(\"   c) Context Information (during expansion):\")\n",
    "print(\"      - Connected array shapes and types\")\n",
    "print(\"      - Parent SDFG structure\")\n",
    "print(\"      - Target execution environment\")\n",
    "print()\n",
    "\n",
    "print(\"3. IMPLEMENTATION REGISTRY:\")\n",
    "print(\"   - Multiple concrete implementations per abstract operation\")\n",
    "print(\"   - Implementation selection based on:\")\n",
    "print(\"     * Explicit specification (node.implementation)\")\n",
    "print(\"     * Default implementation\")\n",
    "print(\"     * Configuration overrides\") \n",
    "print(\"     * Library defaults\")\n",
    "print()\n",
    "\n",
    "print(\"4. EXPANSION PROCESS:\")\n",
    "print(\"   - LibraryNode → Concrete SDFG subgraph\")\n",
    "print(\"   - Access to all compile-time facts\")\n",
    "print(\"   - Can generate optimized code based on parameters\")\n",
    "print(\"   - Environment-specific implementations\")\n",
    "print()\n",
    "\n",
    "print(\"5. BENEFITS OF THIS SYSTEM:\")\n",
    "print(\"   - Clear separation of 'what' vs 'how'\")\n",
    "print(\"   - Compile-time optimization opportunities\")\n",
    "print(\"   - Portable algorithms across targets\")\n",
    "print(\"   - Extensible library ecosystem\")\n",
    "print(\"   - Type safety and validation\")\n",
    "\n",
    "# Demonstrate the key facts available at different stages\n",
    "print(f\"\\n=== Example: Facts Available for Our VectorAdd ===\")\n",
    "example_node = VectorAdd(\"demo\", n=1024, scale_factor=1.5)\n",
    "\n",
    "print(f\"Semantic facts:\")\n",
    "print(f\"  - Operation: Vector addition with scaling\")\n",
    "print(f\"  - Parameters: n={example_node.n}, scale={example_node.scale_factor}\")\n",
    "print(f\"  - Interface: {len(example_node.in_connectors)} inputs, {len(example_node.out_connectors)} outputs\")\n",
    "print(f\"  - Available implementations: {list(VectorAdd.implementations.keys())}\")\n",
    "print(f\"  - Default implementation: {VectorAdd.default_implementation}\")\n",
    "\n",
    "print(f\"\\nThese facts enable:\")\n",
    "print(f\"  - Compile-time validation\")\n",
    "print(f\"  - Implementation selection\")  \n",
    "print(f\"  - Code generation optimization\")\n",
    "print(f\"  - Cross-platform portability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fbe8fa",
   "metadata": {},
   "source": [
    "## Next Steps: Advanced Topics in DaCe Library Nodes\n",
    "\n",
    "Now that we understand the basic LibraryNode system, we can explore more advanced topics:\n",
    "\n",
    "1. **Advanced Library Nodes**: Create complex operations like matrix inversion, FFT, or convolutions\n",
    "2. **Environment-specific Implementations**: Optimize for different hardware (CPU, GPU, FPGA)\n",
    "3. **Library Integration**: Connect to external libraries like MKL, cuBLAS, or TensorFlow\n",
    "4. **Optimization Transformations**: Apply high-level transformations using the semantic information\n",
    "5. **Front-end Integration**: Connect Python functions to automatically generate library nodes\n",
    "\n",
    "The key insight is that LibraryNodes establish a **semantic contract** at the IR level, capturing \"what\" an operation does through compile-time facts, while the implementation system handles \"how\" it gets executed on specific targets. This separation enables both performance portability and optimization opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893b225",
   "metadata": {},
   "source": [
    "## Using this wheel in another environment\n",
    "\n",
    "Here are the exact checks and overrides you likely want when installing the wheel in a fresh environment that may have its own wheels/libs already:\n",
    "\n",
    "\n",
    "\n",
    "1) Install the wheel and BLAS/LAPACK runtime\n",
    "\n",
    "- If using conda:\n",
    "\n",
    "  - `conda install libblas libcblas liblapack openblas` (or `mkl` on x86 if you prefer MKL)\n",
    "\n",
    "- Then install the built wheel:\n",
    "\n",
    "  - `pip install /path/to/dist/dace-1.0.0-py3-none-any.whl`\n",
    "\n",
    "\n",
    "\n",
    "2) Verify BLAS/LAPACK are discoverable (like cell 14)\n",
    "\n",
    "- This lists key libraries and shows what `ctypes.find_library` resolves:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "147d4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11\n",
      "CONDA prefix: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev\n",
      "Listing known BLAS/LAPACK libs under /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/lib\n",
      "  libblas*: 2\n",
      "   - libblas.3.dylib\n",
      "   - libblas.dylib\n",
      "  libcblas*: 2\n",
      "   - libcblas.3.dylib\n",
      "   - libcblas.dylib\n",
      "  liblapack*: 2\n",
      "   - liblapack.3.dylib\n",
      "   - liblapack.dylib\n",
      "  liblapacke*: 0\n",
      "  libopenblas*: 7\n",
      "   - libopenblas.0.dylib\n",
      "   - libopenblas.a\n",
      "   - libopenblas.dylib\n",
      "   - libopenblas_armv8p-r0.3.30.dylib\n",
      "   - libopenblas_vortexp-r0.3.30.a\n",
      "   ...\n",
      "  libmkl*: 0\n",
      "\n",
      "ctypes.find_library...\n",
      "  blas: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/../lib/libblas.dylib\n",
      "  cblas: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/../lib/libcblas.dylib\n",
      "  lapacke: None\n",
      "  lapack: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/../lib/liblapack.dylib\n",
      "  openblas: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/../lib/libopenblas.dylib\n",
      "  mkl_rt: None\n"
     ]
    }
   ],
   "source": [
    "# Verify BLAS/LAPACK availability and ctypes resolution\n",
    "\n",
    "import ctypes.util, os, sys, glob\n",
    "\n",
    "print('Python:', sys.version.split()[0])\n",
    "\n",
    "cp = os.environ.get('CONDA_PREFIX', '(no conda)')\n",
    "\n",
    "print('CONDA prefix:', cp)\n",
    "\n",
    "lib = os.path.join(cp, 'lib') if cp != '(no conda)' else None\n",
    "\n",
    "if lib and os.path.isdir(lib):\n",
    "\n",
    "    print('Listing known BLAS/LAPACK libs under', lib)\n",
    "\n",
    "    for pat in ['libblas*','libcblas*','liblapack*','liblapacke*','libopenblas*','libmkl*']:\n",
    "\n",
    "        matches = sorted(glob.glob(os.path.join(lib, pat)))\n",
    "\n",
    "        print(f'  {pat}:', len(matches))\n",
    "\n",
    "        for m in matches[:5]:\n",
    "\n",
    "            print('   -', os.path.basename(m))\n",
    "\n",
    "        if len(matches) > 5:\n",
    "\n",
    "            print('   ...')\n",
    "\n",
    "print('\\nctypes.find_library...')\n",
    "\n",
    "for name in ['blas','cblas','lapacke','lapack','openblas','mkl_rt']:\n",
    "\n",
    "    print(f'  {name}:', ctypes.util.find_library(name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a7d9c",
   "metadata": {},
   "source": [
    "3) Pin compilers and CMake args (like the config in cell 17)\n",
    "\n",
    "- If you want CMake to use compilers from your env, set CC/CXX or provide extra CMake args.\n",
    "\n",
    "- The snippet below mirrors what you captured (OpenBLAS vendor + conda include/lib paths).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf90607",
   "metadata": {},
   "source": [
    "### Option A: set environment variables (before starting Python)\n",
    "\n",
    "`export CC=\"$CONDA_PREFIX/bin/clang\"`\n",
    "\n",
    "`export CXX=\"$CONDA_PREFIX/bin/clang++\"\n",
    "\n",
    "\n",
    "\n",
    "### Option B: set DaCe extra CMake args programmatically\n",
    "\n",
    "from dace.config import Config\n",
    "\n",
    "extra = \"-DBLA_VENDOR=OpenBLAS -DCMAKE_FIND_FRAMEWORK=LAST \" \\\n",
    "\n",
    "        \"-DCMAKE_INCLUDE_PATH=${CONDA_PREFIX}/include \" \\\n",
    "\n",
    "        \"-DCMAKE_LIBRARY_PATH=${CONDA_PREFIX}/lib\"\n",
    "\n",
    "Config.set('compiler', 'extra_cmake_args', extra)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8502b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra_cmake_args: -DBLA_VENDOR=OpenBLAS -DCMAKE_FIND_FRAMEWORK=LAST -DCMAKE_INCLUDE_PATH=${CONDA_PREFIX}/include -DCMAKE_LIBRARY_PATH=${CONDA_PREFIX}/lib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('extra_cmake_args:', Config.get('compiler','extra_cmake_args'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf8213",
   "metadata": {},
   "source": [
    "4) Confirm which tools are on PATH (noisy shell inits avoided)\n",
    "\n",
    "- Use Python to find where llvm/clang live without invoking a login shell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7177e562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llvm-config: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/llvm-config\n",
      "clang: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/clang\n",
      "clang++: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/clang++\n",
      "mlir-opt: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/mlir-opt\n",
      "mlir-translate: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev/bin/mlir-translate\n",
      "CONDA_PREFIX: /opt/homebrew/Caskroom/miniforge/base/envs/dace-dev\n"
     ]
    }
   ],
   "source": [
    "# Resolve tool locations from Python\n",
    "\n",
    "import shutil, os\n",
    "\n",
    "for tool in ['llvm-config','clang','clang++','mlir-opt','mlir-translate']:\n",
    "\n",
    "    print(f'{tool}:', shutil.which(tool))\n",
    "\n",
    "print('CONDA_PREFIX:', os.environ.get('CONDA_PREFIX'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f012d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dace-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
